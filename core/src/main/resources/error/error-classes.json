{
  "AMBIGUOUS_COLUMN_OR_FIELD" : {
    "message" : [
      "Column or field <name> is ambiguous and has <n> matches."
    ],
    "sqlState" : "42000"
  },
  "ARITHMETIC_OVERFLOW" : {
    "message" : [
      "<message>.<alternative> If necessary set <config> to \"false\" to bypass this error."
    ],
    "sqlState" : "22003"
  },
  "CANNOT_CAST_DATATYPE" : {
    "message" : [
      "Cannot cast <sourceType> to <targetType>."
    ],
    "sqlState" : "22005"
  },
  "CANNOT_CONSTRUCT_PROTOBUF_DESCRIPTOR" : {
    "message" : [
      "Error constructing FileDescriptor for <descFilePath>"
    ]
  },
  "CANNOT_CONVERT_PROTOBUF_FIELD_TYPE_TO_SQL_TYPE" : {
    "message" : [
      "Cannot convert Protobuf <protobufColumn> to SQL <sqlColumn> because schema is incompatible (protobufType = <protobufType>, sqlType = <sqlType>)."
    ]
  },
  "CANNOT_CONVERT_PROTOBUF_MESSAGE_TYPE_TO_SQL_TYPE" : {
    "message" : [
      "Unable to convert <protobufType> of Protobuf to SQL type <toType>."
    ]
  },
  "CANNOT_CONVERT_SQL_TYPE_TO_PROTOBUF_ENUM_TYPE" : {
    "message" : [
      "Cannot convert SQL <sqlColumn> to Protobuf <protobufColumn> because <data> cannot be written since it's not defined in ENUM <enumString>"
    ]
  },
  "CANNOT_CONVERT_SQL_TYPE_TO_PROTOBUF_FIELD_TYPE" : {
    "message" : [
      "Cannot convert SQL <sqlColumn> to Protobuf <protobufColumn> because schema is incompatible (protobufType = <protobufType>, sqlType = <sqlType>)."
    ]
  },
  "CANNOT_DECODE_URL" : {
    "message" : [
      "Cannot decode url : <url>."
    ],
    "sqlState" : "42000"
  },
  "CANNOT_LOAD_PROTOBUF_CLASS" : {
    "message" : [
      "Could not load Protobuf class with name <protobufClassName>. Ensure the class name includes package prefix."
    ]
  },
  "CANNOT_PARSE_DECIMAL" : {
    "message" : [
      "Cannot parse decimal"
    ],
    "sqlState" : "42000"
  },
  "CANNOT_PARSE_PROTOBUF_DESCRIPTOR" : {
    "message" : [
      "Error parsing file <descFilePath> descriptor byte[] into Descriptor object"
    ]
  },
  "CANNOT_PARSE_TIMESTAMP" : {
    "message" : [
      "<message>. If necessary set <ansiConfig> to \"false\" to bypass this error."
    ],
    "sqlState" : "42000"
  },
  "CANNOT_UP_CAST_DATATYPE" : {
    "message" : [
      "Cannot up cast <expression> from <sourceType> to <targetType>.",
      "<details>"
    ]
  },
  "CAST_INVALID_INPUT" : {
    "message" : [
      "The value <expression> of the type <sourceType> cannot be cast to <targetType> because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set <ansiConfig> to \"false\" to bypass this error."
    ],
    "sqlState" : "42000"
  },
  "CAST_OVERFLOW" : {
    "message" : [
      "The value <value> of the type <sourceType> cannot be cast to <targetType> due to an overflow. Use `try_cast` to tolerate overflow and return NULL instead. If necessary set <ansiConfig> to \"false\" to bypass this error."
    ],
    "sqlState" : "22005"
  },
  "CAST_OVERFLOW_IN_TABLE_INSERT" : {
    "message" : [
      "Fail to insert a value of <sourceType> type into the <targetType> type column <columnName> due to an overflow. Use `try_cast` on the input value to tolerate overflow and return NULL instead."
    ],
    "sqlState" : "22005"
  },
  "COLUMN_NOT_IN_GROUP_BY_CLAUSE" : {
    "message" : [
      "The expression <expression> is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in `first()` (or `first_value()`) if you don't care which value you get."
    ],
    "sqlState" : "42000"
  },
  "CONCURRENT_QUERY" : {
    "message" : [
      "Another instance of this query was just started by a concurrent session."
    ]
  },
  "CONNECT" : {
    "message" : [
      "Generic Spark Connect error."
    ],
    "subClass" : {
      "INTERCEPTOR_CTOR_MISSING" : {
        "message" : [
          "Cannot instantiate GRPC interceptor because <cls> is missing a default constructor without arguments."
        ]
      },
      "INTERCEPTOR_RUNTIME_ERROR" : {
        "message" : [
          "Error instantiating GRPC interceptor: <msg>"
        ]
      }
    }
  },
  "CONVERSION_INVALID_INPUT" : {
    "message" : [
      "The value <str> (<fmt>) cannot be converted to <targetType> because it is malformed. Correct the value as per the syntax, or change its format. Use <suggestion> to tolerate malformed input and return NULL instead."
    ]
  },
  "CREATE_TABLE_COLUMN_OPTION_DUPLICATE" : {
    "message" : [
      "CREATE TABLE column <columnName> specifies option \"<optionName>\" more than once, which is invalid"
    ]
  },
  "DATATYPE_MISMATCH" : {
    "message" : [
      "Cannot resolve <sqlExpr> due to data type mismatch:"
    ],
    "subClass" : {
      "BINARY_ARRAY_DIFF_TYPES" : {
        "message" : [
          "Input to function <functionName> should have been two <arrayType> with same element type, but it's [<leftType>, <rightType>]."
        ]
      },
      "BINARY_OP_DIFF_TYPES" : {
        "message" : [
          "the left and right operands of the binary operator have incompatible types (<left> and <right>)."
        ]
      },
      "BINARY_OP_WRONG_TYPE" : {
        "message" : [
          "the binary operator requires the input type <inputType>, not <actualDataType>."
        ]
      },
      "BLOOM_FILTER_BINARY_OP_WRONG_TYPE" : {
        "message" : [
          "The Bloom filter binary input to <functionName> should be either a constant value or a scalar subquery expression, but it's <actual>."
        ]
      },
      "BLOOM_FILTER_WRONG_TYPE" : {
        "message" : [
          "Input to function <functionName> should have been <expectedLeft> followed by value with <expectedRight>, but it's [<actual>]."
        ]
      },
      "CANNOT_CONVERT_TO_JSON" : {
        "message" : [
          "Unable to convert column <name> of type <type> to JSON."
        ]
      },
      "CANNOT_DROP_ALL_FIELDS" : {
        "message" : [
          "Cannot drop all fields in struct."
        ]
      },
      "CAST_WITHOUT_SUGGESTION" : {
        "message" : [
          "cannot cast <srcType> to <targetType>."
        ]
      },
      "CAST_WITH_CONF_SUGGESTION" : {
        "message" : [
          "cannot cast <srcType> to <targetType> with ANSI mode on.",
          "If you have to cast <srcType> to <targetType>, you can set <config> as <configVal>."
        ]
      },
      "CAST_WITH_FUN_SUGGESTION" : {
        "message" : [
          "cannot cast <srcType> to <targetType>.",
          "To convert values from <srcType> to <targetType>, you can use the functions <functionNames> instead."
        ]
      },
      "CREATE_MAP_KEY_DIFF_TYPES" : {
        "message" : [
          "The given keys of function <functionName> should all be the same type, but they are <dataType>."
        ]
      },
      "CREATE_MAP_VALUE_DIFF_TYPES" : {
        "message" : [
          "The given values of function <functionName> should all be the same type, but they are <dataType>."
        ]
      },
      "CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING" : {
        "message" : [
          "Only foldable `STRING` expressions are allowed to appear at odd position, but they are <inputExprs>."
        ]
      },
      "DATA_DIFF_TYPES" : {
        "message" : [
          "Input to <functionName> should all be the same type, but it's <dataType>."
        ]
      },
      "HASH_MAP_TYPE" : {
        "message" : [
          "Input to the function <functionName> cannot contain elements of the \"MAP\" type. In Spark, same maps may have different hashcode, thus hash expressions are prohibited on \"MAP\" elements. To restore previous behavior set \"spark.sql.legacy.allowHashOnMapType\" to \"true\"."
        ]
      },
      "INVALID_JSON_MAP_KEY_TYPE" : {
        "message" : [
          "Input schema <schema> can only contain STRING as a key type for a MAP."
        ]
      },
      "INVALID_JSON_SCHEMA" : {
        "message" : [
          "Input schema <schema> must be a struct, an array or a map."
        ]
      },
      "INVALID_MAP_KEY_TYPE" : {
        "message" : [
          "The key of map cannot be/contain <keyType>."
        ]
      },
      "INVALID_ORDERING_TYPE" : {
        "message" : [
          "The <functionName> does not support ordering on type <dataType>."
        ]
      },
      "IN_SUBQUERY_DATA_TYPE_MISMATCH" : {
        "message" : [
          "The data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [<mismatchedColumns>], left side: [<leftType>], right side: [<rightType>]."
        ]
      },
      "IN_SUBQUERY_LENGTH_MISMATCH" : {
        "message" : [
          "The number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length: <leftLength>): [<leftColumns>], right hand side columns(length: <rightLength>): [<rightColumns>]."
        ]
      },
      "MAP_CONCAT_DIFF_TYPES" : {
        "message" : [
          "The <functionName> should all be of type map, but it's <dataType>."
        ]
      },
      "MAP_CONTAINS_KEY_DIFF_TYPES" : {
        "message" : [
          "Input to <functionName> should have been <dataType> followed by a value with same key type, but it's [<leftType>, <rightType>]."
        ]
      },
      "MAP_ZIP_WITH_DIFF_TYPES" : {
        "message" : [
          "Input to the <functionName> should have been two maps with compatible key types, but it's [<leftType>, <rightType>]."
        ]
      },
      "NON_FOLDABLE_INPUT" : {
        "message" : [
          "the input <inputName> should be a foldable <inputType> expression; however, got <inputExpr>."
        ]
      },
      "NON_STRING_TYPE" : {
        "message" : [
          "all arguments must be strings."
        ]
      },
      "NULL_TYPE" : {
        "message" : [
          "Null typed values cannot be used as arguments of <functionName>."
        ]
      },
      "PARAMETER_CONSTRAINT_VIOLATION" : {
        "message" : [
          "The <leftExprName>(<leftExprValue>) must be <constraint> the <rightExprName>(<rightExprValue>)"
        ]
      },
      "RANGE_FRAME_INVALID_TYPE" : {
        "message" : [
          "The data type <orderSpecType> used in the order specification does not match the data type <valueBoundaryType> which is used in the range frame."
        ]
      },
      "RANGE_FRAME_MULTI_ORDER" : {
        "message" : [
          "A range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: <orderSpec>."
        ]
      },
      "RANGE_FRAME_WITHOUT_ORDER" : {
        "message" : [
          "A range window frame cannot be used in an unordered window specification."
        ]
      },
      "SPECIFIED_WINDOW_FRAME_DIFF_TYPES" : {
        "message" : [
          "Window frame bounds <lower> and <upper> do not have the same type: <lowerType> <> <upperType>."
        ]
      },
      "SPECIFIED_WINDOW_FRAME_INVALID_BOUND" : {
        "message" : [
          "Window frame upper bound <upper> does not follow the lower bound <lower>."
        ]
      },
      "SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE" : {
        "message" : [
          "The data type of the <location> bound <exprType> does not match the expected data type <expectedType>."
        ]
      },
      "SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE" : {
        "message" : [
          "Window frame <location> bound <expression> is not a literal."
        ]
      },
      "SPECIFIED_WINDOW_FRAME_WRONG_COMPARISON" : {
        "message" : [
          "The lower bound of a window frame must be <comparison> to the upper bound."
        ]
      },
      "STACK_COLUMN_DIFF_TYPES" : {
        "message" : [
          "The data type of the column (<columnIndex>) do not have the same type: <leftType> (<leftParamIndex>) <> <rightType> (<rightParamIndex>)."
        ]
      },
      "UNEXPECTED_CLASS_TYPE" : {
        "message" : [
          "class <className> not found"
        ]
      },
      "UNEXPECTED_INPUT_TYPE" : {
        "message" : [
          "parameter <paramIndex> requires <requiredType> type, however, <inputSql> is of <inputType> type."
        ]
      },
      "UNEXPECTED_NULL" : {
        "message" : [
          "The <exprName> must not be null"
        ]
      },
      "UNEXPECTED_RETURN_TYPE" : {
        "message" : [
          "The <functionName> requires return <expectedType> type, but the actual is <actualType> type."
        ]
      },
      "UNEXPECTED_STATIC_METHOD" : {
        "message" : [
          "cannot find a static method <methodName> that matches the argument types in <className>"
        ]
      },
      "UNSPECIFIED_FRAME" : {
        "message" : [
          "Cannot use an UnspecifiedFrame. This should have been converted during analysis."
        ]
      },
      "UNSUPPORTED_INPUT_TYPE" : {
        "message" : [
          "The input of <functionName> can't be <dataType> type data."
        ]
      },
      "VALUE_OUT_OF_RANGE" : {
        "message" : [
          "The <exprName> must be between <valueRange> (current value = <currentValue>)"
        ]
      },
      "WRONG_NUM_ARGS" : {
        "message" : [
          "The <functionName> requires <expectedNum> parameters but the actual number is <actualNum>."
        ]
      },
      "WRONG_NUM_ARGS_WITH_SUGGESTION" : {
        "message" : [
          "The <functionName> requires <expectedNum> parameters but the actual number is <actualNum>.",
          "If you have to call this function with <legacyNum> parameters, set the legacy configuration <legacyConfKey> to <legacyConfValue>."
        ]
      },
      "WRONG_NUM_ENDPOINTS" : {
        "message" : [
          "The number of endpoints must be >= 2 to construct intervals but the actual number is <actualNumber>."
        ]
      }
    }
  },
  "DATATYPE_MISSING_SIZE" : {
    "message" : [
      "DataType <type> requires a length parameter, for example <type>(10). Please specify the length."
    ],
    "sqlState" : "42000"
  },
  "DATETIME_OVERFLOW" : {
    "message" : [
      "Datetime operation overflow: <operation>."
    ],
    "sqlState" : "22008"
  },
  "DECIMAL_PRECISION_EXCEEDS_MAX_PRECISION" : {
    "message" : [
      "Decimal precision <precision> exceeds max precision <maxPrecision>."
    ]
  },
  "DEFAULT_DATABASE_NOT_EXISTS" : {
    "message" : [
      "Default database <defaultDatabase> does not exist, please create it first or change default database to 'default'."
    ]
  },
  "DIVIDE_BY_ZERO" : {
    "message" : [
      "Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set <config> to \"false\" to bypass this error."
    ],
    "sqlState" : "22012"
  },
  "DUPLICATE_KEY" : {
    "message" : [
      "Found duplicate keys <keyColumn>"
    ],
    "sqlState" : "23000"
  },
  "ELEMENT_AT_BY_INDEX_ZERO" : {
    "message" : [
      "The index 0 is invalid. An index shall be either < 0 or > 0 (the first element has index 1)."
    ]
  },
  "FAILED_EXECUTE_UDF" : {
    "message" : [
      "Failed to execute user defined function (<functionName>: (<signature>) => <result>)"
    ]
  },
  "FAILED_RENAME_PATH" : {
    "message" : [
      "Failed to rename <sourcePath> to <targetPath> as destination already exists"
    ],
    "sqlState" : "22023"
  },
  "FIELD_NOT_FOUND" : {
    "message" : [
      "No such struct field <fieldName> in <fields>."
    ]
  },
  "FORBIDDEN_OPERATION" : {
    "message" : [
      "The operation <statement> is not allowed on the <objectType>: <objectName>"
    ]
  },
  "GRAPHITE_SINK_INVALID_PROTOCOL" : {
    "message" : [
      "Invalid Graphite protocol: <protocol>"
    ]
  },
  "GRAPHITE_SINK_PROPERTY_MISSING" : {
    "message" : [
      "Graphite sink requires '<property>' property."
    ]
  },
  "GROUPING_COLUMN_MISMATCH" : {
    "message" : [
      "Column of grouping (<grouping>) can't be found in grouping columns <groupingColumns>"
    ],
    "sqlState" : "42000"
  },
  "GROUPING_ID_COLUMN_MISMATCH" : {
    "message" : [
      "Columns of grouping_id (<groupingIdColumn>) does not match grouping columns (<groupByColumns>)"
    ],
    "sqlState" : "42000"
  },
  "GROUPING_SIZE_LIMIT_EXCEEDED" : {
    "message" : [
      "Grouping sets size cannot be greater than <maxSize>"
    ]
  },
  "GROUP_BY_POS_OUT_OF_RANGE" : {
    "message" : [
      "GROUP BY position <index> is not in select list (valid range is [1, <size>])."
    ],
    "sqlState" : "42000"
  },
  "GROUP_BY_POS_REFERS_AGG_EXPR" : {
    "message" : [
      "GROUP BY <index> refers to an expression <aggExpr> that contains an aggregate function. Aggregate functions are not allowed in GROUP BY."
    ],
    "sqlState" : "42000"
  },
  "INCOMPARABLE_PIVOT_COLUMN" : {
    "message" : [
      "Invalid pivot column <columnName>. Pivot columns must be comparable."
    ],
    "sqlState" : "42000"
  },
  "INCOMPATIBLE_DATASOURCE_REGISTER" : {
    "message" : [
      "Detected an incompatible DataSourceRegister. Please remove the incompatible library from classpath or upgrade it. Error: <message>"
    ]
  },
  "INCONSISTENT_BEHAVIOR_CROSS_VERSION" : {
    "message" : [
      "You may get a different result due to the upgrading to"
    ],
    "subClass" : {
      "DATETIME_PATTERN_RECOGNITION" : {
        "message" : [
          "Spark >= 3.0:",
          "Fail to recognize <pattern> pattern in the DateTimeFormatter. 1) You can set <config> to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html"
        ]
      },
      "PARSE_DATETIME_BY_NEW_PARSER" : {
        "message" : [
          "Spark >= 3.0:",
          "Fail to parse <datetime> in the new parser. You can set <config> to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string."
        ]
      },
      "READ_ANCIENT_DATETIME" : {
        "message" : [
          "Spark >= 3.0:",
          "reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z",
          "from <format> files can be ambiguous, as the files may be written by",
          "Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar",
          "that is different from Spark 3.0+'s Proleptic Gregorian calendar.",
          "See more details in SPARK-31404. You can set the SQL config <config> or",
          "the datasource option <option> to \"LEGACY\" to rebase the datetime values",
          "w.r.t. the calendar difference during reading. To read the datetime values",
          "as it is, set the SQL config or the datasource option to \"CORRECTED\"."
        ]
      },
      "WRITE_ANCIENT_DATETIME" : {
        "message" : [
          "Spark >= 3.0:",
          "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z",
          "into <format> files can be dangerous, as the files may be read by Spark 2.x",
          "or legacy versions of Hive later, which uses a legacy hybrid calendar that",
          "is different from Spark 3.0+'s Proleptic Gregorian calendar. See more",
          "details in SPARK-31404. You can set <config> to \"LEGACY\" to rebase the",
          "datetime values w.r.t. the calendar difference during writing, to get maximum",
          "interoperability. Or set the config to \"CORRECTED\" to write the datetime",
          "values as it is, if you are sure that the written files will only be read by",
          "Spark 3.0+ or other systems that use Proleptic Gregorian calendar."
        ]
      }
    }
  },
  "INCORRECT_END_OFFSET" : {
    "message" : [
      "Max offset with <rowsPerSecond> rowsPerSecond is <maxSeconds>, but it's <endSeconds> now."
    ]
  },
  "INCORRECT_RAMP_UP_RATE" : {
    "message" : [
      "Max offset with <rowsPerSecond> rowsPerSecond is <maxSeconds>, but 'rampUpTimeSeconds' is <rampUpTimeSeconds>."
    ]
  },
  "INDEX_ALREADY_EXISTS" : {
    "message" : [
      "Cannot create the index because it already exists. <message>."
    ],
    "sqlState" : "42000"
  },
  "INDEX_NOT_FOUND" : {
    "message" : [
      "Cannot find the index. <message>."
    ],
    "sqlState" : "42000"
  },
  "INTERNAL_ERROR" : {
    "message" : [
      "<message>"
    ]
  },
  "INTERVAL_ARITHMETIC_OVERFLOW" : {
    "message" : [
      "<message>.<alternative>"
    ],
    "sqlState" : "22003"
  },
  "INTERVAL_DIVIDED_BY_ZERO" : {
    "message" : [
      "Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead."
    ],
    "sqlState" : "22012"
  },
  "INVALID_ARRAY_INDEX" : {
    "message" : [
      "The index <indexValue> is out of bounds. The array has <arraySize> elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set <ansiConfig> to \"false\" to bypass this error."
    ]
  },
  "INVALID_ARRAY_INDEX_IN_ELEMENT_AT" : {
    "message" : [
      "The index <indexValue> is out of bounds. The array has <arraySize> elements. Use `try_element_at` to tolerate accessing element at invalid index and return NULL instead. If necessary set <ansiConfig> to \"false\" to bypass this error."
    ]
  },
  "INVALID_BUCKET_FILE" : {
    "message" : [
      "Invalid bucket file: <path>"
    ]
  },
  "INVALID_BYTE_STRING" : {
    "message" : [
      "The expected format is ByteString, but was <unsupported> (<class>)."
    ]
  },
  "INVALID_COLUMN_OR_FIELD_DATA_TYPE" : {
    "message" : [
      "Column or field <name> is of type <type> while it's required to be <expectedType>."
    ],
    "sqlState" : "42000"
  },
  "INVALID_FIELD_NAME" : {
    "message" : [
      "Field name <fieldName> is invalid: <path> is not a struct."
    ],
    "sqlState" : "42000"
  },
  "INVALID_FRACTION_OF_SECOND" : {
    "message" : [
      "The fraction of sec must be zero. Valid range is [0, 60]. If necessary set <ansiConfig> to \"false\" to bypass this error."
    ],
    "sqlState" : "22023"
  },
  "INVALID_IDENTIFIER" : {
    "message" : [
      "The identifier <ident> is invalid. Please, consider quoting it with back-quotes as `<ident>`."
    ]
  },
  "INVALID_JSON_SCHEMA_MAP_TYPE" : {
    "message" : [
      "Input schema <jsonSchema> can only contain STRING as a key type for a MAP."
    ]
  },
  "INVALID_PANDAS_UDF_PLACEMENT" : {
    "message" : [
      "The group aggregate pandas UDF <functionList> cannot be invoked together with as other, non-pandas aggregate functions."
    ]
  },
  "INVALID_PARAMETER_VALUE" : {
    "message" : [
      "The value of parameter(s) '<parameter>' in <functionName> is invalid: <expected>"
    ],
    "sqlState" : "22023"
  },
  "INVALID_PROPERTY_KEY" : {
    "message" : [
      "<key> is an invalid property key, please use quotes, e.g. SET <key>=<value>"
    ]
  },
  "INVALID_PROPERTY_VALUE" : {
    "message" : [
      "<value> is an invalid property value, please use quotes, e.g. SET <key>=<value>"
    ]
  },
  "INVALID_PROTOBUF_MESSAGE_TYPE" : {
    "message" : [
      "<protobufClassName> is not a Protobuf message type"
    ]
  },
  "INVALID_SQL_SYNTAX" : {
    "message" : [
      "Invalid SQL syntax: <inputString>"
    ],
    "sqlState" : "42000"
  },
  "INVALID_SUBQUERY_EXPRESSION" : {
    "message" : [
      "Invalid subquery:"
    ],
    "subClass" : {
      "SCALAR_SUBQUERY_RETURN_MORE_THAN_ONE_OUTPUT_COLUMN" : {
        "message" : [
          "Scalar subquery must return only one column, but got <number>"
        ]
      }
    }
  },
  "LOCATION_ALREADY_EXISTS" : {
    "message" : [
      "Cannot name the managed table as <identifier>, as its associated location <location> already exists. Please pick a different table name, or remove the existing location first."
    ]
  },
  "MALFORMED_PROTOBUF_MESSAGE" : {
    "message" : [
      "Malformed Protobuf messages are detected in message deserialization. Parse Mode: <failFastMode>. To process malformed protobuf message as null result, try setting the option 'mode' as 'PERMISSIVE'."
    ]
  },
  "MISSING_STATIC_PARTITION_COLUMN" : {
    "message" : [
      "Unknown static partition column: <columnName>"
    ],
    "sqlState" : "42000"
  },
  "MULTI_UDF_INTERFACE_ERROR" : {
    "message" : [
      "Not allowed to implement multiple UDF interfaces, UDF class <className>"
    ]
  },
  "NON_LAST_MATCHED_CLAUSE_OMIT_CONDITION" : {
    "message" : [
      "When there are more than one MATCHED clauses in a MERGE statement, only the last MATCHED clause can omit the condition."
    ],
    "sqlState" : "42000"
  },
  "NON_LAST_NOT_MATCHED_BY_SOURCE_CLAUSE_OMIT_CONDITION" : {
    "message" : [
      "When there are more than one NOT MATCHED BY SOURCE clauses in a MERGE statement, only the last NOT MATCHED BY SOURCE clause can omit the condition."
    ],
    "sqlState" : "42000"
  },
  "NON_LAST_NOT_MATCHED_BY_TARGET_CLAUSE_OMIT_CONDITION" : {
    "message" : [
      "When there are more than one NOT MATCHED [BY TARGET] clauses in a MERGE statement, only the last NOT MATCHED [BY TARGET] clause can omit the condition."
    ],
    "sqlState" : "42000"
  },
  "NON_LITERAL_PIVOT_VALUES" : {
    "message" : [
      "Literal expressions required for pivot values, found <expression>."
    ],
    "sqlState" : "42000"
  },
  "NON_PARTITION_COLUMN" : {
    "message" : [
      "PARTITION clause cannot contain the non-partition column: <columnName>."
    ],
    "sqlState" : "42000"
  },
  "NO_HANDLER_FOR_UDAF" : {
    "message" : [
      "No handler for UDAF '<functionName>'. Use sparkSession.udf.register(...) instead."
    ]
  },
  "NO_SQL_TYPE_IN_PROTOBUF_SCHEMA" : {
    "message" : [
      "Cannot find <catalystFieldPath> in Protobuf schema"
    ]
  },
  "NO_UDF_INTERFACE" : {
    "message" : [
      "UDF class <className> doesn't implement any UDF interface"
    ]
  },
  "NULLABLE_ARRAY_OR_MAP_ELEMENT" : {
    "message" : [
      "Array or map at <columnPath> contains nullable element while it's required to be non-nullable."
    ],
    "sqlState" : "42000"
  },
  "NULLABLE_COLUMN_OR_FIELD" : {
    "message" : [
      "Column or field <name> is nullable while it's required to be non-nullable."
    ],
    "sqlState" : "42000"
  },
  "NULL_COMPARISON_RESULT" : {
    "message" : [
      "The comparison result is null. If you want to handle null as 0 (equal), you can set \"spark.sql.legacy.allowNullComparisonResultInArraySort\" to \"true\"."
    ]
  },
  "NUMERIC_VALUE_OUT_OF_RANGE" : {
    "message" : [
      "<value> cannot be represented as Decimal(<precision>, <scale>). If necessary set <config> to \"false\" to bypass this error."
    ],
    "sqlState" : "22005"
  },
  "NUM_COLUMNS_MISMATCH" : {
    "message" : [
      "<operator> can only be performed on tables with the same number of columns, but the first table has <refNumColumns> columns and the <invalidOrdinalNum> table has <invalidNumColumns> columns."
    ]
  },
  "ORDER_BY_POS_OUT_OF_RANGE" : {
    "message" : [
      "ORDER BY position <index> is not in select list (valid range is [1, <size>])."
    ],
    "sqlState" : "42000"
  },
  "OUT_OF_DECIMAL_TYPE_RANGE" : {
    "message" : [
      "Out of decimal type range: <value>."
    ]
  },
  "PARSE_EMPTY_STATEMENT" : {
    "message" : [
      "Syntax error, unexpected empty statement"
    ],
    "sqlState" : "42000"
  },
  "PARSE_SYNTAX_ERROR" : {
    "message" : [
      "Syntax error at or near <error><hint>"
    ],
    "sqlState" : "42000"
  },
  "PARTITIONS_ALREADY_EXIST" : {
    "message" : [
      "Cannot ADD or RENAME TO partition(s) <partitionList> in table <tableName> because they already exist.",
      "Choose a different name, drop the existing partition, or add the IF NOT EXISTS clause to tolerate a pre-existing partition."
    ],
    "sqlState" : "42000"
  },
  "PARTITIONS_NOT_FOUND" : {
    "message" : [
      "The partition(s) <partitionList> cannot be found in table <tableName>.",
      "Verify the partition specification and table name.",
      "To tolerate the error on drop use ALTER TABLE … DROP IF EXISTS PARTITION."
    ],
    "sqlState" : "42000"
  },
  "PIVOT_VALUE_DATA_TYPE_MISMATCH" : {
    "message" : [
      "Invalid pivot value '<value>': value data type <valueType> does not match pivot column data type <pivotType>"
    ],
    "sqlState" : "42000"
  },
  "PROTOBUF_DEPENDENCY_NOT_FOUND" : {
    "message" : [
      "Could not find dependency: <dependencyName>"
    ]
  },
  "PROTOBUF_DESCRIPTOR_FILE_NOT_FOUND" : {
    "message" : [
      "Error reading Protobuf descriptor file at path: <filePath>"
    ]
  },
  "PROTOBUF_FIELD_MISSING" : {
    "message" : [
      "Searching for <field> in Protobuf schema at <protobufSchema> gave <matchSize> matches. Candidates: <matches>"
    ]
  },
  "PROTOBUF_FIELD_MISSING_IN_SQL_SCHEMA" : {
    "message" : [
      "Found <field> in Protobuf schema but there is no match in the SQL schema"
    ]
  },
  "PROTOBUF_FIELD_TYPE_MISMATCH" : {
    "message" : [
      "Type mismatch encountered for field: <field>"
    ]
  },
  "PROTOBUF_MESSAGE_NOT_FOUND" : {
    "message" : [
      "Unable to locate Message <messageName> in Descriptor"
    ]
  },
  "PROTOBUF_TYPE_NOT_SUPPORT" : {
    "message" : [
      "Protobuf type not yet supported: <protobufType>."
    ]
  },
  "RECURSIVE_PROTOBUF_SCHEMA" : {
    "message" : [
      "Found recursive reference in Protobuf schema, which can not be processed by Spark: <fieldDescriptor>"
    ]
  },
  "RENAME_SRC_PATH_NOT_FOUND" : {
    "message" : [
      "Failed to rename as <sourcePath> was not found"
    ],
    "sqlState" : "22023"
  },
  "RESET_PERMISSION_TO_ORIGINAL" : {
    "message" : [
      "Failed to set original permission <permission> back to the created path: <path>. Exception: <message>"
    ]
  },
  "ROUTINE_ALREADY_EXISTS" : {
    "message" : [
      "Cannot create the function <routineName> because it already exists.",
      "Choose a different name, drop or replace the existing function, or add the IF NOT EXISTS clause to tolerate a pre-existing function."
    ],
    "sqlState" : "42000"
  },
  "ROUTINE_NOT_FOUND" : {
    "message" : [
      "The function <routineName> cannot be found. Verify the spelling and correctness of the schema and catalog.",
      "If you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog.",
      "To tolerate the error on drop use DROP FUNCTION IF EXISTS."
    ],
    "sqlState" : "42000"
  },
  "SCALAR_SUBQUERY_TOO_MANY_ROWS" : {
    "message" : [
      "More than one row returned by a subquery used as an expression."
    ]
  },
  "SCHEMA_ALREADY_EXISTS" : {
    "message" : [
      "Cannot create schema <schemaName> because it already exists.",
      "Choose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema."
    ],
    "sqlState" : "42000"
  },
  "SCHEMA_NOT_EMPTY" : {
    "message" : [
      "Cannot drop a schema <schemaName> because it contains objects.",
      "Use DROP SCHEMA ... CASCADE to drop the schema and all its objects."
    ],
    "sqlState" : "42000"
  },
  "SCHEMA_NOT_FOUND" : {
    "message" : [
      "The schema <schemaName> cannot be found. Verify the spelling and correctness of the schema and catalog.",
      "If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.",
      "To tolerate the error on drop use DROP SCHEMA IF EXISTS."
    ],
    "sqlState" : "42000"
  },
  "SECOND_FUNCTION_ARGUMENT_NOT_INTEGER" : {
    "message" : [
      "The second argument of <functionName> function needs to be an integer."
    ],
    "sqlState" : "22023"
  },
  "STAR_GROUP_BY_POS" : {
    "message" : [
      "Star (*) is not allowed in a select list when GROUP BY an ordinal position is used."
    ]
  },
  "TABLE_OR_VIEW_ALREADY_EXISTS" : {
    "message" : [
      "Cannot create table or view <relationName> because it already exists.",
      "Choose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects."
    ],
    "sqlState" : "42000"
  },
  "TABLE_OR_VIEW_NOT_FOUND" : {
    "message" : [
      "The table or view <relationName> cannot be found. Verify the spelling and correctness of the schema and catalog.",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS."
    ],
    "sqlState" : "42000"
  },
  "TEMP_TABLE_OR_VIEW_ALREADY_EXISTS" : {
    "message" : [
      "Cannot create the temporary view <relationName> because it already exists.",
      "Choose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views."
    ],
    "sqlState" : "42000"
  },
  "TOO_MANY_ARRAY_ELEMENTS" : {
    "message" : [
      "Cannot initialize array with <numElements> elements of size <size>"
    ]
  },
  "UNABLE_TO_ACQUIRE_MEMORY" : {
    "message" : [
      "Unable to acquire <requestedBytes> bytes of memory, got <receivedBytes>"
    ]
  },
  "UNABLE_TO_CONVERT_TO_PROTOBUF_MESSAGE_TYPE" : {
    "message" : [
      "Unable to convert SQL type <toType> to Protobuf type <protobufType>."
    ]
  },
  "UNCLOSED_BRACKETED_COMMENT" : {
    "message" : [
      "Found an unclosed bracketed comment. Please, append */ at the end of the comment."
    ]
  },
  "UNKNOWN_PROTOBUF_MESSAGE_TYPE" : {
    "message" : [
      "Attempting to treat <descriptorName> as a Message, but it was <containingType>"
    ]
  },
  "UNPIVOT_REQUIRES_ATTRIBUTES" : {
    "message" : [
      "UNPIVOT requires all given <given> expressions to be columns when no <empty> expressions are given. These are not columns: [<expressions>]."
    ],
    "sqlState" : "42000"
  },
  "UNPIVOT_REQUIRES_VALUE_COLUMNS" : {
    "message" : [
      "At least one value column needs to be specified for UNPIVOT, all columns specified as ids"
    ],
    "sqlState" : "42000"
  },
  "UNPIVOT_VALUE_DATA_TYPE_MISMATCH" : {
    "message" : [
      "Unpivot value columns must share a least common type, some types do not: [<types>]"
    ],
    "sqlState" : "42000"
  },
  "UNPIVOT_VALUE_SIZE_MISMATCH" : {
    "message" : [
      "All unpivot value columns must have the same size as there are value column names (<names>)"
    ],
    "sqlState" : "42000"
  },
  "UNRECOGNIZED_SQL_TYPE" : {
    "message" : [
      "Unrecognized SQL type <typeName>"
    ],
    "sqlState" : "42000"
  },
  "UNRESOLVED_COLUMN" : {
    "message" : [
      "A column or function parameter with name <objectName> cannot be resolved."
    ],
    "subClass" : {
      "WITHOUT_SUGGESTION" : {
        "message" : [
          ""
        ]
      },
      "WITH_SUGGESTION" : {
        "message" : [
          "Did you mean one of the following? [<proposal>]"
        ]
      }
    },
    "sqlState" : "42000"
  },
  "UNRESOLVED_FIELD" : {
    "message" : [
      "A field with name <fieldName> cannot be resolved with the struct-type column <columnPath>."
    ],
    "subClass" : {
      "WITHOUT_SUGGESTION" : {
        "message" : [
          ""
        ]
      },
      "WITH_SUGGESTION" : {
        "message" : [
          "Did you mean one of the following? [<proposal>]"
        ]
      }
    },
    "sqlState" : "42000"
  },
  "UNRESOLVED_MAP_KEY" : {
    "message" : [
      "Cannot resolve column <objectName> as a map key. If the key is a string literal, please add single quotes around it."
    ],
    "subClass" : {
      "WITHOUT_SUGGESTION" : {
        "message" : [
          ""
        ]
      },
      "WITH_SUGGESTION" : {
        "message" : [
          "Otherwise did you mean one of the following column(s)? [<proposal>]"
        ]
      }
    },
    "sqlState" : "42000"
  },
  "UNSCALED_VALUE_TOO_LARGE_FOR_PRECISION" : {
    "message" : [
      "Unscaled value too large for precision. If necessary set <ansiConfig> to false to bypass this error."
    ]
  },
  "UNSUPPORTED_DATATYPE" : {
    "message" : [
      "Unsupported data type <typeName>"
    ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_DESERIALIZER" : {
    "message" : [
      "The deserializer is not supported:"
    ],
    "subClass" : {
      "DATA_TYPE_MISMATCH" : {
        "message" : [
          "need a(n) <desiredType> field but got <dataType>."
        ]
      },
      "FIELD_NUMBER_MISMATCH" : {
        "message" : [
          "try to map <schema> to Tuple<ordinal>, but failed as the number of fields does not line up."
        ]
      }
    }
  },
  "UNSUPPORTED_EMPTY_LOCATION" : {
    "message" : [
      "Unsupported empty location."
    ]
  },
  "UNSUPPORTED_FEATURE" : {
    "message" : [
      "The feature is not supported:"
    ],
    "subClass" : {
      "AES_MODE" : {
        "message" : [
          "AES-<mode> with the padding <padding> by the <functionName> function."
        ]
      },
      "CATALOG_OPERATION" : {
        "message" : [
          "Catalog <catalogName> does not support <operation>."
        ]
      },
      "DESC_TABLE_COLUMN_PARTITION" : {
        "message" : [
          "DESC TABLE COLUMN for a specific partition."
        ]
      },
      "DISTRIBUTE_BY" : {
        "message" : [
          "DISTRIBUTE BY clause."
        ]
      },
      "INSERT_PARTITION_SPEC_IF_NOT_EXISTS" : {
        "message" : [
          "INSERT INTO <tableName> IF NOT EXISTS in the PARTITION spec."
        ]
      },
      "JDBC_TRANSACTION" : {
        "message" : [
          "The target JDBC server does not support transactions and can only support ALTER TABLE with a single action."
        ]
      },
      "LATERAL_JOIN_OF_TYPE" : {
        "message" : [
          "<joinType> JOIN with LATERAL correlation."
        ]
      },
      "LATERAL_JOIN_USING" : {
        "message" : [
          "JOIN USING with LATERAL correlation."
        ]
      },
      "LATERAL_NATURAL_JOIN" : {
        "message" : [
          "NATURAL join with LATERAL correlation."
        ]
      },
      "LITERAL_TYPE" : {
        "message" : [
          "Literal for '<value>' of <type>."
        ]
      },
      "MULTIPLE_BUCKET_TRANSFORMS" : {
        "message" : [
          "Multiple bucket TRANSFORMs."
        ]
      },
      "NATURAL_CROSS_JOIN" : {
        "message" : [
          "NATURAL CROSS JOIN."
        ]
      },
      "ORC_TYPE_CAST" : {
        "message" : [
          "Unable to convert <orcType> of Orc to data type <toType>."
        ]
      },
      "PANDAS_UDAF_IN_PIVOT" : {
        "message" : [
          "Pandas user defined aggregate function in the PIVOT clause."
        ]
      },
      "PIVOT_AFTER_GROUP_BY" : {
        "message" : [
          "PIVOT clause following a GROUP BY clause."
        ]
      },
      "PIVOT_TYPE" : {
        "message" : [
          "Pivoting by the value '<value>' of the column data type <type>."
        ]
      },
      "PYTHON_UDF_IN_ON_CLAUSE" : {
        "message" : [
          "Python UDF in the ON clause of a <joinType> JOIN."
        ]
      },
      "REPEATED_PIVOT" : {
        "message" : [
          "Repeated PIVOT operation."
        ]
      },
      "SET_NAMESPACE_PROPERTY" : {
        "message" : [
          "<property> is a reserved namespace property, <msg>."
        ]
      },
      "SET_PROPERTIES_AND_DBPROPERTIES" : {
        "message" : [
          "set PROPERTIES and DBPROPERTIES at the same time."
        ]
      },
      "SET_TABLE_PROPERTY" : {
        "message" : [
          "<property> is a reserved table property, <msg>."
        ]
      },
      "TABLE_OPERATION" : {
        "message" : [
          "Table <tableName> does not support <operation>. Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog implementation which is configured by \"spark.sql.catalog\"."
        ]
      },
      "TOO_MANY_TYPE_ARGUMENTS_FOR_UDF_CLASS" : {
        "message" : [
          "UDF class with <num> type arguments."
        ]
      },
      "TRANSFORM_DISTINCT_ALL" : {
        "message" : [
          "TRANSFORM with the DISTINCT/ALL clause."
        ]
      },
      "TRANSFORM_NON_HIVE" : {
        "message" : [
          "TRANSFORM with SERDE is only supported in hive mode."
        ]
      }
    },
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_GENERATOR" : {
    "message" : [
      "The generator is not supported:"
    ],
    "subClass" : {
      "MULTI_GENERATOR" : {
        "message" : [
          "only one generator allowed per <clause> clause but found <num>: <generators>"
        ]
      },
      "NESTED_IN_EXPRESSIONS" : {
        "message" : [
          "nested in expressions <expression>"
        ]
      },
      "NOT_GENERATOR" : {
        "message" : [
          "<functionName> is expected to be a generator. However, its class is <classCanonicalName>, which is not a generator."
        ]
      },
      "OUTSIDE_SELECT" : {
        "message" : [
          "outside the SELECT clause, found: <plan>"
        ]
      }
    }
  },
  "UNSUPPORTED_GROUPING_EXPRESSION" : {
    "message" : [
      "grouping()/grouping_id() can only be used with GroupingSets/Cube/Rollup"
    ]
  },
  "UNSUPPORTED_SAVE_MODE" : {
    "message" : [
      "The save mode <saveMode> is not supported for:"
    ],
    "subClass" : {
      "EXISTENT_PATH" : {
        "message" : [
          "an existent path."
        ]
      },
      "NON_EXISTENT_PATH" : {
        "message" : [
          "a non-existent path."
        ]
      }
    }
  },
  "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY" : {
    "message" : [
      "Unsupported subquery expression:"
    ],
    "subClass" : {
      "ACCESSING_OUTER_QUERY_COLUMN_IS_NOT_ALLOWED" : {
        "message" : [
          "Accessing outer query column is not allowed in this location<treeNode>"
        ]
      },
      "AGGREGATE_FUNCTION_MIXED_OUTER_LOCAL_REFERENCES" : {
        "message" : [
          "Found an aggregate function in a correlated predicate that has both outer and local references, which is not supported: <function>"
        ]
      },
      "CORRELATED_COLUMN_IS_NOT_ALLOWED_IN_PREDICATE" : {
        "message" : [
          "Correlated column is not allowed in predicate: <treeNode>"
        ]
      },
      "CORRELATED_COLUMN_NOT_FOUND" : {
        "message" : [
          "A correlated outer name reference within a subquery expression body was not found in the enclosing query: <value>"
        ]
      },
      "LATERAL_JOIN_CONDITION_NON_DETERMINISTIC" : {
        "message" : [
          "Lateral join condition cannot be non-deterministic: <condition>"
        ]
      },
      "MUST_AGGREGATE_CORRELATED_SCALAR_SUBQUERY" : {
        "message" : [
          "Correlated scalar subqueries in the GROUP BY clause must also be in the aggregate expressions<treeNode>"
        ]
      },
      "MUST_AGGREGATE_CORRELATED_SCALAR_SUBQUERY_OUTPUT" : {
        "message" : [
          "The output of a correlated scalar subquery must be aggregated"
        ]
      },
      "NON_CORRELATED_COLUMNS_IN_GROUP_BY" : {
        "message" : [
          "A GROUP BY clause in a scalar correlated subquery cannot contain non-correlated columns: <value>"
        ]
      },
      "NON_DETERMINISTIC_LATERAL_SUBQUERIES" : {
        "message" : [
          "Non-deterministic lateral subqueries are not supported when joining with outer relations that produce more than one row<treeNode>"
        ]
      },
      "UNSUPPORTED_CORRELATED_REFERENCE" : {
        "message" : [
          "Expressions referencing the outer query are not supported outside of WHERE/HAVING clauses<treeNode>"
        ]
      },
      "UNSUPPORTED_CORRELATED_REFERENCE_DATA_TYPE" : {
        "message" : [
          "Correlated column reference '<expr>' cannot be <dataType> type"
        ]
      },
      "UNSUPPORTED_CORRELATED_SCALAR_SUBQUERY" : {
        "message" : [
          "Correlated scalar subqueries can only be used in filters, aggregations, projections, and UPDATE/MERGE/DELETE commands<treeNode>"
        ]
      },
      "UNSUPPORTED_IN_EXISTS_SUBQUERY" : {
        "message" : [
          "IN/EXISTS predicate subqueries can only be used in filters, joins, aggregations, window functions, projections, and UPDATE/MERGE/DELETE commands<treeNode>"
        ]
      }
    }
  },
  "UNSUPPORTED_TYPED_LITERAL" : {
    "message" : [
      "Literals of the type <unsupportedType> are not supported. Supported types are <supportedTypes>."
    ]
  },
  "UNTYPED_SCALA_UDF" : {
    "message" : [
      "You're using untyped Scala UDF, which does not have the input type information. Spark may blindly pass null to the Scala closure with primitive-type argument, and the closure will see the default value of the Java type for the null argument, e.g. `udf((x: Int) => x, IntegerType)`, the result is 0 for null input. To get rid of this error, you could:",
      "1. use typed Scala UDF APIs(without return type parameter), e.g. `udf((x: Int) => x)`",
      "2. use Java UDF APIs, e.g. `udf(new UDF1[String, Integer] { override def call(s: String): Integer = s.length() }, IntegerType)`, if input types are all non primitive",
      "3. set \"spark.sql.legacy.allowUntypedScalaUDF\" to \"true\" and use this API with caution"
    ]
  },
  "_LEGACY_ERROR_TEMP_0001" : {
    "message" : [
      "Invalid InsertIntoContext"
    ]
  },
  "_LEGACY_ERROR_TEMP_0002" : {
    "message" : [
      "INSERT OVERWRITE DIRECTORY is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_0003" : {
    "message" : [
      "Columns aliases are not allowed in <op>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0004" : {
    "message" : [
      "Empty source for merge: you should specify a source table/subquery in merge."
    ]
  },
  "_LEGACY_ERROR_TEMP_0006" : {
    "message" : [
      "The number of inserted values cannot match the fields."
    ]
  },
  "_LEGACY_ERROR_TEMP_0008" : {
    "message" : [
      "There must be at least one WHEN clause in a MERGE statement."
    ]
  },
  "_LEGACY_ERROR_TEMP_0011" : {
    "message" : [
      "Combination of ORDER BY/SORT BY/DISTRIBUTE BY/CLUSTER BY is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_0012" : {
    "message" : [
      "DISTRIBUTE BY is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_0013" : {
    "message" : [
      "LATERAL cannot be used together with PIVOT in FROM clause."
    ]
  },
  "_LEGACY_ERROR_TEMP_0014" : {
    "message" : [
      "TABLESAMPLE does not accept empty inputs."
    ]
  },
  "_LEGACY_ERROR_TEMP_0015" : {
    "message" : [
      "TABLESAMPLE(<msg>) is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_0016" : {
    "message" : [
      "<bytesStr> is not a valid byte length literal, expected syntax: DIGIT+ ('B' | 'K' | 'M' | 'G')."
    ]
  },
  "_LEGACY_ERROR_TEMP_0017" : {
    "message" : [
      "Invalid escape string. Escape string must contain only one character."
    ]
  },
  "_LEGACY_ERROR_TEMP_0018" : {
    "message" : [
      "Function trim doesn't support with type <trimOption>. Please use BOTH, LEADING or TRAILING as trim type."
    ]
  },
  "_LEGACY_ERROR_TEMP_0019" : {
    "message" : [
      "Cannot parse the <valueType> value: <value>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0020" : {
    "message" : [
      "Cannot parse the INTERVAL value: <value>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0022" : {
    "message" : [
      "<msg>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0023" : {
    "message" : [
      "Numeric literal <rawStrippedQualifier> does not fit in range [<minValue>, <maxValue>] for type <typeName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0024" : {
    "message" : [
      "Can only have a single from-to unit in the interval literal syntax."
    ]
  },
  "_LEGACY_ERROR_TEMP_0025" : {
    "message" : [
      "At least one time unit should be given for interval literal."
    ]
  },
  "_LEGACY_ERROR_TEMP_0026" : {
    "message" : [
      "Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: <value>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0027" : {
    "message" : [
      "The value of from-to unit must be a string."
    ]
  },
  "_LEGACY_ERROR_TEMP_0028" : {
    "message" : [
      "Intervals FROM <from> TO <to> are not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_0029" : {
    "message" : [
      "Cannot mix year-month and day-time fields: <literal>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0030" : {
    "message" : [
      "DataType <dataType> is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_0031" : {
    "message" : [
      "Invalid number of buckets: <describe>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0032" : {
    "message" : [
      "Duplicated table paths found: '<pathOne>' and '<pathTwo>'. LOCATION and the case insensitive key 'path' in OPTIONS are all used to indicate the custom table path, you can only specify one of them."
    ]
  },
  "_LEGACY_ERROR_TEMP_0033" : {
    "message" : [
      "Expected either STORED AS or STORED BY, not both."
    ]
  },
  "_LEGACY_ERROR_TEMP_0034" : {
    "message" : [
      "<operation> is not supported in Hive-style <command><msg>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0035" : {
    "message" : [
      "Operation not allowed: <message>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0036" : {
    "message" : [
      "Expected `NOSCAN` instead of `<ctx>`."
    ]
  },
  "_LEGACY_ERROR_TEMP_0037" : {
    "message" : [
      "It is not allowed to add catalog/namespace prefix <quoted> to the table name in CACHE TABLE AS SELECT."
    ]
  },
  "_LEGACY_ERROR_TEMP_0038" : {
    "message" : [
      "CTE definition can't have duplicate names: <duplicateNames>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0039" : {
    "message" : [
      "Unsupported SQL statement."
    ]
  },
  "_LEGACY_ERROR_TEMP_0041" : {
    "message" : [
      "Found duplicate clauses: <clauseName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0042" : {
    "message" : [
      "Expected format is 'SET', 'SET key', or 'SET key=value'. If you want to include special characters in key, or include semicolon in value, please use quotes, e.g., SET `key`=`value`."
    ]
  },
  "_LEGACY_ERROR_TEMP_0043" : {
    "message" : [
      "Expected format is 'RESET' or 'RESET key'. If you want to include special characters in key, please use quotes, e.g., RESET `key`."
    ]
  },
  "_LEGACY_ERROR_TEMP_0044" : {
    "message" : [
      "The interval value must be in the range of [-18, +18] hours with second precision."
    ]
  },
  "_LEGACY_ERROR_TEMP_0045" : {
    "message" : [
      "Invalid time zone displacement value."
    ]
  },
  "_LEGACY_ERROR_TEMP_0046" : {
    "message" : [
      "CREATE TEMPORARY TABLE without a provider is not allowed."
    ]
  },
  "_LEGACY_ERROR_TEMP_0047" : {
    "message" : [
      "'ROW FORMAT' must be used with 'STORED AS'."
    ]
  },
  "_LEGACY_ERROR_TEMP_0048" : {
    "message" : [
      "Unsupported operation: Used defined record reader/writer classes."
    ]
  },
  "_LEGACY_ERROR_TEMP_0049" : {
    "message" : [
      "Directory path and 'path' in OPTIONS should be specified one, but not both."
    ]
  },
  "_LEGACY_ERROR_TEMP_0050" : {
    "message" : [
      "LOCAL is supported only with file: scheme."
    ]
  },
  "_LEGACY_ERROR_TEMP_0051" : {
    "message" : [
      "Empty set in <element> grouping sets is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_0052" : {
    "message" : [
      "CREATE VIEW with both IF NOT EXISTS and REPLACE is not allowed."
    ]
  },
  "_LEGACY_ERROR_TEMP_0053" : {
    "message" : [
      "It is not allowed to define a TEMPORARY view with IF NOT EXISTS."
    ]
  },
  "_LEGACY_ERROR_TEMP_0054" : {
    "message" : [
      "It is not allowed to add database prefix `<database>` for the TEMPORARY view name."
    ]
  },
  "_LEGACY_ERROR_TEMP_0056" : {
    "message" : [
      "Invalid time travel spec: <reason>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0057" : {
    "message" : [
      "Support for DEFAULT column values is not implemented yet."
    ]
  },
  "_LEGACY_ERROR_TEMP_0058" : {
    "message" : [
      "Support for DEFAULT column values is not allowed."
    ]
  },
  "_LEGACY_ERROR_TEMP_0059" : {
    "message" : [
      "References to DEFAULT column values are not allowed within the PARTITION clause."
    ]
  },
  "_LEGACY_ERROR_TEMP_0060" : {
    "message" : [
      "<msg>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0061" : {
    "message" : [
      "<msg>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0062" : {
    "message" : [
      "<msg>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0063" : {
    "message" : [
      "<msg>."
    ]
  },
  "_LEGACY_ERROR_TEMP_0064" : {
    "message" : [
      "<msg>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1000" : {
    "message" : [
      "LEGACY store assignment policy is disallowed in Spark data source V2. Please set the configuration <configKey> to other values."
    ]
  },
  "_LEGACY_ERROR_TEMP_1001" : {
    "message" : [
      "USING column `<colName>` cannot be resolved on the <side> side of the join. The <side>-side columns: [<plan>]."
    ]
  },
  "_LEGACY_ERROR_TEMP_1002" : {
    "message" : [
      "Unable to generate an encoder for inner class `<className>` without access to the scope that this class was defined in.",
      "Try moving this class out of its parent class."
    ]
  },
  "_LEGACY_ERROR_TEMP_1003" : {
    "message" : [
      "Couldn't find the reference column for <after> at <parentName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1004" : {
    "message" : [
      "Window specification <windowName> is not defined in the WINDOW clause."
    ]
  },
  "_LEGACY_ERROR_TEMP_1005" : {
    "message" : [
      "<expr> doesn't show up in the GROUP BY list <groupByAliases>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1006" : {
    "message" : [
      "Aggregate expression required for pivot, but '<sql>' did not appear in any aggregate function."
    ]
  },
  "_LEGACY_ERROR_TEMP_1007" : {
    "message" : [
      "Cannot write into temp view <quoted> as it's not a data source v2 relation."
    ]
  },
  "_LEGACY_ERROR_TEMP_1008" : {
    "message" : [
      "<quoted> is not a temp view of streaming logical plan, please use batch API such as `DataFrameReader.table` to read it."
    ]
  },
  "_LEGACY_ERROR_TEMP_1009" : {
    "message" : [
      "The depth of view <identifier> exceeds the maximum view resolution depth (<maxNestedViewDepth>). Analysis is aborted to avoid errors. Increase the value of <config> to work around this."
    ]
  },
  "_LEGACY_ERROR_TEMP_1010" : {
    "message" : [
      "Inserting into a view is not allowed. View: <identifier>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1011" : {
    "message" : [
      "Writing into a view is not allowed. View: <identifier>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1012" : {
    "message" : [
      "Cannot write into v1 table: <identifier>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1013" : {
    "message" : [
      "<nameParts> is a <viewStr>. '<cmd>' expects a table.<hintStr>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1014" : {
    "message" : [
      "<nameParts> is a temp view. '<cmd>' expects a permanent view."
    ]
  },
  "_LEGACY_ERROR_TEMP_1015" : {
    "message" : [
      "<identifier> is a table. '<cmd>' expects a view.<hintStr>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1016" : {
    "message" : [
      "<nameParts> is a temp view. '<cmd>' expects a table or permanent view."
    ]
  },
  "_LEGACY_ERROR_TEMP_1017" : {
    "message" : [
      "<name> is a built-in/temporary function. '<cmd>' expects a persistent function.<hintStr>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1018" : {
    "message" : [
      "<quoted> is a permanent view, which is not supported by streaming reading API such as `DataStreamReader.table` yet."
    ]
  },
  "_LEGACY_ERROR_TEMP_1020" : {
    "message" : [
      "Invalid usage of <elem> in <prettyName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1021" : {
    "message" : [
      "count(<targetString>.*) is not allowed. Please use count(*) or expand the columns manually, e.g. count(col1, col2)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1023" : {
    "message" : [
      "Function <prettyName> does not support <syntax>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1024" : {
    "message" : [
      "FILTER expression is non-deterministic, it cannot be used in aggregate functions."
    ]
  },
  "_LEGACY_ERROR_TEMP_1025" : {
    "message" : [
      "FILTER expression is not of type boolean. It cannot be used in an aggregate function."
    ]
  },
  "_LEGACY_ERROR_TEMP_1026" : {
    "message" : [
      "FILTER expression contains aggregate. It cannot be used in an aggregate function."
    ]
  },
  "_LEGACY_ERROR_TEMP_1027" : {
    "message" : [
      "FILTER expression contains window function. It cannot be used in an aggregate function."
    ]
  },
  "_LEGACY_ERROR_TEMP_1028" : {
    "message" : [
      "Number of column aliases does not match number of columns. Number of column aliases: <columnSize>; number of columns: <outputSize>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1029" : {
    "message" : [
      "The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected <aliasesSize> aliases but got <aliasesNames>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1030" : {
    "message" : [
      "Window aggregate function with filter predicate is not supported yet."
    ]
  },
  "_LEGACY_ERROR_TEMP_1031" : {
    "message" : [
      "It is not allowed to use a window function inside an aggregate function. Please use the inner window function in a sub-query."
    ]
  },
  "_LEGACY_ERROR_TEMP_1032" : {
    "message" : [
      "<expr> does not have any WindowExpression."
    ]
  },
  "_LEGACY_ERROR_TEMP_1033" : {
    "message" : [
      "<expr> has multiple Window Specifications (<distinctWindowSpec>).",
      "Please file a bug report with this error message, stack trace, and the query."
    ]
  },
  "_LEGACY_ERROR_TEMP_1034" : {
    "message" : [
      "It is not allowed to use window functions inside <clauseName> clause."
    ]
  },
  "_LEGACY_ERROR_TEMP_1035" : {
    "message" : [
      "Cannot specify window frame for <prettyName> function."
    ]
  },
  "_LEGACY_ERROR_TEMP_1036" : {
    "message" : [
      "Window Frame <wf> must match the required frame <required>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1037" : {
    "message" : [
      "Window function <wf> requires window to be ordered, please add ORDER BY clause. For example SELECT <wf>(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table."
    ]
  },
  "_LEGACY_ERROR_TEMP_1038" : {
    "message" : [
      "Cannot write to table due to mismatched user specified column size(<columnSize>) and data column size(<outputSize>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1039" : {
    "message" : [
      "Multiple time/session window expressions would result in a cartesian product of rows, therefore they are currently not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_1040" : {
    "message" : [
      "Gap duration expression used in session window must be CalendarIntervalType, but got <dt>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1041" : {
    "message" : [
      "Undefined function <name>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1042" : {
    "message" : [
      "Invalid number of arguments for function <name>. Expected: <expectedInfo>; Found: <actualNumber>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1043" : {
    "message" : [
      "Invalid arguments for function <name>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1044" : {
    "message" : [
      "Function <name> accepts only one argument."
    ]
  },
  "_LEGACY_ERROR_TEMP_1045" : {
    "message" : [
      "ALTER TABLE SET LOCATION does not support partition for v2 tables."
    ]
  },
  "_LEGACY_ERROR_TEMP_1046" : {
    "message" : [
      "Join strategy hint parameter should be an identifier or string but was <unsupported> (<class>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1047" : {
    "message" : [
      "<hintName> Hint parameter should include columns, but <invalidParams> found."
    ]
  },
  "_LEGACY_ERROR_TEMP_1048" : {
    "message" : [
      "<hintName> Hint expects a partition number as a parameter."
    ]
  },
  "_LEGACY_ERROR_TEMP_1049" : {
    "message" : [
      "Syntax error in attribute name: <name>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1050" : {
    "message" : [
      "Can only star expand struct data types. Attribute: `<attributes>`."
    ]
  },
  "_LEGACY_ERROR_TEMP_1051" : {
    "message" : [
      "Cannot resolve '<targetString>.*' given input columns '<columns>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1052" : {
    "message" : [
      "ADD COLUMN with v1 tables cannot specify NOT NULL."
    ]
  },
  "_LEGACY_ERROR_TEMP_1053" : {
    "message" : [
      "ALTER COLUMN with v1 tables cannot specify NOT NULL."
    ]
  },
  "_LEGACY_ERROR_TEMP_1054" : {
    "message" : [
      "ALTER COLUMN cannot find column <colName> in v1 table. Available: <fieldNames>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1055" : {
    "message" : [
      "The database name is not valid: <quoted>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1056" : {
    "message" : [
      "Cannot drop a view with DROP TABLE. Please use DROP VIEW instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1057" : {
    "message" : [
      "SHOW COLUMNS with conflicting databases: '<dbA>' != '<dbB>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1058" : {
    "message" : [
      "Cannot create table with both USING <provider> and <serDeInfo>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1059" : {
    "message" : [
      "STORED AS with file format '<serdeInfo>' is invalid."
    ]
  },
  "_LEGACY_ERROR_TEMP_1060" : {
    "message" : [
      "<command> does not support nested column: <column>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1061" : {
    "message" : [
      "Column <colName> does not exist."
    ]
  },
  "_LEGACY_ERROR_TEMP_1065" : {
    "message" : [
      "`<name>` is not a valid name for tables/databases. Valid names only contain alphabet characters, numbers and _."
    ]
  },
  "_LEGACY_ERROR_TEMP_1066" : {
    "message" : [
      "<database> is a system preserved database, you cannot create a database with this name."
    ]
  },
  "_LEGACY_ERROR_TEMP_1067" : {
    "message" : [
      "Can not drop default database."
    ]
  },
  "_LEGACY_ERROR_TEMP_1068" : {
    "message" : [
      "<database> is a system preserved database, you cannot use it as current database. To access global temporary views, you should use qualified name with the GLOBAL_TEMP_DATABASE, e.g. SELECT * FROM <database>.viewName."
    ]
  },
  "_LEGACY_ERROR_TEMP_1069" : {
    "message" : [
      "CREATE EXTERNAL TABLE must be accompanied by LOCATION."
    ]
  },
  "_LEGACY_ERROR_TEMP_1071" : {
    "message" : [
      "Some existing schema fields (<nonExistentColumnNames>) are not present in the new schema. We don't support dropping columns yet."
    ]
  },
  "_LEGACY_ERROR_TEMP_1072" : {
    "message" : [
      "Only the tables/views belong to the same database can be retrieved. Querying tables/views are <qualifiedTableNames>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1073" : {
    "message" : [
      "RENAME TABLE source and destination databases do not match: '<db>' != '<newDb>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1074" : {
    "message" : [
      "RENAME TEMPORARY VIEW from '<oldName>' to '<newName>': cannot specify database name '<db>' in the destination table."
    ]
  },
  "_LEGACY_ERROR_TEMP_1076" : {
    "message" : [
      "Partition spec is invalid. <details>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1078" : {
    "message" : [
      "Can not load class '<className>' when registering the function '<func>', please make sure it is on the classpath."
    ]
  },
  "_LEGACY_ERROR_TEMP_1079" : {
    "message" : [
      "Resource Type '<resourceType>' is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_1080" : {
    "message" : [
      "Table <identifier> did not specify database."
    ]
  },
  "_LEGACY_ERROR_TEMP_1081" : {
    "message" : [
      "Table <identifier> did not specify locationUri."
    ]
  },
  "_LEGACY_ERROR_TEMP_1082" : {
    "message" : [
      "Partition [<specString>] did not specify locationUri."
    ]
  },
  "_LEGACY_ERROR_TEMP_1083" : {
    "message" : [
      "Number of buckets should be greater than 0 but less than or equal to bucketing.maxBuckets (`<bucketingMaxBuckets>`). Got `<numBuckets>`."
    ]
  },
  "_LEGACY_ERROR_TEMP_1084" : {
    "message" : [
      "Corrupted table name context in catalog: <numParts> parts expected, but part <index> is missing."
    ]
  },
  "_LEGACY_ERROR_TEMP_1085" : {
    "message" : [
      "Corrupted view SQL configs in catalog."
    ]
  },
  "_LEGACY_ERROR_TEMP_1086" : {
    "message" : [
      "Corrupted view query output column names in catalog: <numCols> parts expected, but part <index> is missing."
    ]
  },
  "_LEGACY_ERROR_TEMP_1087" : {
    "message" : [
      "Corrupted view referred temp view names in catalog."
    ]
  },
  "_LEGACY_ERROR_TEMP_1088" : {
    "message" : [
      "Corrupted view referred temp functions names in catalog."
    ]
  },
  "_LEGACY_ERROR_TEMP_1089" : {
    "message" : [
      "Column statistics deserialization is not supported for column <name> of data type: <dataType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1090" : {
    "message" : [
      "Column statistics serialization is not supported for column <colName> of data type: <dataType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1091" : {
    "message" : [
      "Cannot read table property '<key>' as it's corrupted.<details>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1092" : {
    "message" : [
      "The expression '<expr>' is not a valid schema string."
    ]
  },
  "_LEGACY_ERROR_TEMP_1093" : {
    "message" : [
      "Schema should be specified in DDL format as a string literal or output of the schema_of_json/schema_of_csv functions instead of <expr>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1094" : {
    "message" : [
      "Schema should be struct type but got <dataType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1095" : {
    "message" : [
      "A type of keys and values in map() must be string, but got <map>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1096" : {
    "message" : [
      "Must use a map() function for options."
    ]
  },
  "_LEGACY_ERROR_TEMP_1097" : {
    "message" : [
      "The field for corrupt records must be string type and nullable."
    ]
  },
  "_LEGACY_ERROR_TEMP_1098" : {
    "message" : [
      "DataType '<x>' is not supported by <className>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1099" : {
    "message" : [
      "<funcName>() doesn't support the <mode> mode. Acceptable modes are <permissiveMode> and <failFastMode>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1100" : {
    "message" : [
      "The '<argName>' parameter of function '<funcName>' needs to be a <requiredType> literal."
    ]
  },
  "_LEGACY_ERROR_TEMP_1101" : {
    "message" : [
      "Invalid value for the '<argName>' parameter of function '<funcName>': <invalidValue>.<endingMsg>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1102" : {
    "message" : [
      "Literals of type '<field>' are currently not supported for the <srcDataType> type."
    ]
  },
  "_LEGACY_ERROR_TEMP_1103" : {
    "message" : [
      "Unsupported component type <clz> in arrays."
    ]
  },
  "_LEGACY_ERROR_TEMP_1104" : {
    "message" : [
      "The second argument should be a double literal."
    ]
  },
  "_LEGACY_ERROR_TEMP_1105" : {
    "message" : [
      "Field name should be String Literal, but it's <extraction>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1106" : {
    "message" : [
      "Can't extract value from <child>: need struct type but got <other>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1107" : {
    "message" : [
      "Table <table> declares <batchWrite> capability but <v2WriteClassName> is not an instance of <v1WriteClassName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1108" : {
    "message" : [
      "Delete by condition with subquery is not supported: <condition>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1109" : {
    "message" : [
      "Exec update failed: cannot translate expression to source filter: <f>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1110" : {
    "message" : [
      "Cannot delete from table <table> where <filters>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1111" : {
    "message" : [
      "DESCRIBE does not support partition for v2 tables."
    ]
  },
  "_LEGACY_ERROR_TEMP_1113" : {
    "message" : [
      "Table <table> does not support <cmd>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1114" : {
    "message" : [
      "The streaming sources in a query do not have a common supported execution mode.",
      "Sources support micro-batch: <microBatchSources>",
      "Sources support continuous: <continuousSources>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1117" : {
    "message" : [
      "<sessionCatalog> requires a single-part namespace, but got <ns>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1119" : {
    "message" : [
      "<cmd> is not supported in JDBC catalog."
    ]
  },
  "_LEGACY_ERROR_TEMP_1120" : {
    "message" : [
      "Unsupported NamespaceChange <changes> in JDBC catalog."
    ]
  },
  "_LEGACY_ERROR_TEMP_1121" : {
    "message" : [
      "Table does not support <cmd>: <table>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1122" : {
    "message" : [
      "Table <table> is not a row-level operation table."
    ]
  },
  "_LEGACY_ERROR_TEMP_1123" : {
    "message" : [
      "Cannot rename a table with ALTER VIEW. Please use ALTER TABLE instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1124" : {
    "message" : [
      "<cmd> is not supported for v2 tables."
    ]
  },
  "_LEGACY_ERROR_TEMP_1125" : {
    "message" : [
      "Database from v1 session catalog is not specified."
    ]
  },
  "_LEGACY_ERROR_TEMP_1126" : {
    "message" : [
      "Nested databases are not supported by v1 session catalog: <catalog>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1127" : {
    "message" : [
      "Invalid partitionExprs specified: <sortOrders> For range partitioning use REPARTITION_BY_RANGE instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1128" : {
    "message" : [
      "Failed to resolve the schema for <format> for the partition column: <partitionColumn>. It must be specified manually."
    ]
  },
  "_LEGACY_ERROR_TEMP_1129" : {
    "message" : [
      "Unable to infer schema for <format>. It must be specified manually."
    ]
  },
  "_LEGACY_ERROR_TEMP_1130" : {
    "message" : [
      "Path does not exist: <path>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1131" : {
    "message" : [
      "Data source <className> does not support <outputMode> output mode."
    ]
  },
  "_LEGACY_ERROR_TEMP_1132" : {
    "message" : [
      "A schema needs to be specified when using <className>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1133" : {
    "message" : [
      "The user-specified schema doesn't match the actual schema:",
      "user-specified: <schema>, actual: <actualSchema>. If you're using",
      "DataFrameReader.schema API or creating a table, please do not specify the schema.",
      "Or if you're scanning an existed table, please drop it and re-create it."
    ]
  },
  "_LEGACY_ERROR_TEMP_1134" : {
    "message" : [
      "Unable to infer schema for <format> at <fileCatalog>. It must be specified manually."
    ]
  },
  "_LEGACY_ERROR_TEMP_1135" : {
    "message" : [
      "<className> is not a valid Spark SQL Data Source."
    ]
  },
  "_LEGACY_ERROR_TEMP_1136" : {
    "message" : [
      "Cannot save interval data type into external storage."
    ]
  },
  "_LEGACY_ERROR_TEMP_1137" : {
    "message" : [
      "Unable to resolve <name> given [<outputStr>]."
    ]
  },
  "_LEGACY_ERROR_TEMP_1138" : {
    "message" : [
      "Hive built-in ORC data source must be used with Hive support enabled. Please use the native ORC data source by setting 'spark.sql.orc.impl' to 'native'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1139" : {
    "message" : [
      "Failed to find data source: <provider>. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide."
    ]
  },
  "_LEGACY_ERROR_TEMP_1140" : {
    "message" : [
      "Failed to find data source: <provider>. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
    ]
  },
  "_LEGACY_ERROR_TEMP_1141" : {
    "message" : [
      "Multiple sources found for <provider> (<sourceNames>), please specify the fully qualified class name."
    ]
  },
  "_LEGACY_ERROR_TEMP_1142" : {
    "message" : [
      "Datasource does not support writing empty or nested empty schemas. Please make sure the data schema has at least one or more column(s)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1143" : {
    "message" : [
      "The data to be inserted needs to have the same number of columns as the target table: target table has <targetSize> column(s) but the inserted data has <actualSize> column(s), which contain <staticPartitionsSize> partition column(s) having assigned constant values."
    ]
  },
  "_LEGACY_ERROR_TEMP_1144" : {
    "message" : [
      "The data to be inserted needs to have the same number of partition columns as the target table: target table has <targetSize> partition column(s) but the inserted data has <providedPartitionsSize> partition columns specified."
    ]
  },
  "_LEGACY_ERROR_TEMP_1145" : {
    "message" : [
      "<partKey> is not a partition column. Partition columns are <partitionColumns>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1146" : {
    "message" : [
      "Partition column <partColumn> have multiple values specified, <values>. Please only specify a single value."
    ]
  },
  "_LEGACY_ERROR_TEMP_1147" : {
    "message" : [
      "The ordering of partition columns is <partColumns>. All partition columns having constant values need to appear before other partition columns that do not have an assigned constant value."
    ]
  },
  "_LEGACY_ERROR_TEMP_1148" : {
    "message" : [
      "Can only write data to relations with a single path."
    ]
  },
  "_LEGACY_ERROR_TEMP_1149" : {
    "message" : [
      "Fail to rebuild expression: missing key <filter> in `translatedFilterToExpr`."
    ]
  },
  "_LEGACY_ERROR_TEMP_1150" : {
    "message" : [
      "Column `<field>` has a data type of <fieldType>, which is not supported by <format>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1151" : {
    "message" : [
      "Fail to resolve data source for the table <table> since the table serde property has the duplicated key <key> with extra options specified for this scan operation. To fix this, you can rollback to the legacy behavior of ignoring the extra options by setting the config <config> to `false`, or address the conflicts of the same config."
    ]
  },
  "_LEGACY_ERROR_TEMP_1152" : {
    "message" : [
      "Path <outputPath> already exists."
    ]
  },
  "_LEGACY_ERROR_TEMP_1153" : {
    "message" : [
      "Cannot use <field> for partition column."
    ]
  },
  "_LEGACY_ERROR_TEMP_1154" : {
    "message" : [
      "Cannot use all columns for partition columns."
    ]
  },
  "_LEGACY_ERROR_TEMP_1155" : {
    "message" : [
      "Partition column `<col>` not found in schema <schemaCatalog>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1156" : {
    "message" : [
      "Column <colName> not found in schema <tableSchema>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1157" : {
    "message" : [
      "Unsupported data source type for direct query on files: <className>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1158" : {
    "message" : [
      "Saving data into a view is not allowed."
    ]
  },
  "_LEGACY_ERROR_TEMP_1159" : {
    "message" : [
      "The format of the existing table <tableName> is `<existingProvider>`. It doesn't match the specified format `<specifiedProvider>`."
    ]
  },
  "_LEGACY_ERROR_TEMP_1160" : {
    "message" : [
      "The location of the existing table <identifier> is `<existingTableLoc>`. It doesn't match the specified location `<tableDescLoc>`."
    ]
  },
  "_LEGACY_ERROR_TEMP_1161" : {
    "message" : [
      "The column number of the existing table <tableName> (<existingTableSchema>) doesn't match the data schema (<querySchema>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1162" : {
    "message" : [
      "Cannot resolve '<col>' given input columns: [<inputColumns>]."
    ]
  },
  "_LEGACY_ERROR_TEMP_1163" : {
    "message" : [
      "Specified partitioning does not match that of the existing table <tableName>.",
      "Specified partition columns: [<specifiedPartCols>]",
      "Existing partition columns: [<existingPartCols>]"
    ]
  },
  "_LEGACY_ERROR_TEMP_1164" : {
    "message" : [
      "Specified bucketing does not match that of the existing table <tableName>.",
      "Specified bucketing: <specifiedBucketString>",
      "Existing bucketing: <existingBucketString>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1165" : {
    "message" : [
      "It is not allowed to specify partitioning when the table schema is not defined."
    ]
  },
  "_LEGACY_ERROR_TEMP_1166" : {
    "message" : [
      "Bucketing column '<bucketCol>' should not be part of partition columns '<normalizedPartCols>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1167" : {
    "message" : [
      "Bucket sorting column '<sortCol>' should not be part of partition columns '<normalizedPartCols>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1168" : {
    "message" : [
      "<tableName> requires that the data to be inserted have the same number of columns as the target table: target table has <targetColumns> column(s) but the inserted data has <insertedColumns> column(s), including <staticPartCols> partition column(s) having constant value(s)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1169" : {
    "message" : [
      "Requested partitioning does not match the table <tableName>:",
      "Requested partitions: <normalizedPartSpec>",
      "Table partitions: <partColNames>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1170" : {
    "message" : [
      "Hive support is required to <detail>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1171" : {
    "message" : [
      "createTableColumnTypes option column <col> not found in schema <schema>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1172" : {
    "message" : [
      "Parquet type not yet supported: <parquetType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1173" : {
    "message" : [
      "Illegal Parquet type: <parquetType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1174" : {
    "message" : [
      "Unrecognized Parquet type: <field>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1175" : {
    "message" : [
      "Unsupported data type <dataType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1176" : {
    "message" : [
      "The SQL query of view <viewName> has an incompatible schema change and column <colName> cannot be resolved. Expected <expectedNum> columns named <colName> but got <actualCols>.",
      "Please try to re-create the view by running: <viewDDL>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1177" : {
    "message" : [
      "The SQL query of view <viewName> has an incompatible schema change and column <colName> cannot be resolved. Expected <expectedNum> columns named <colName> but got <actualCols>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1178" : {
    "message" : [
      "The number of partitions can't be specified with unspecified distribution. Invalid writer requirements detected."
    ]
  },
  "_LEGACY_ERROR_TEMP_1179" : {
    "message" : [
      "Table-valued function <name> with alternatives: <usage>",
      "cannot be applied to (<arguments>): <details>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1180" : {
    "message" : [
      "Incompatible input data type.",
      "Expected: <expectedDataType>; Found: <foundDataType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1181" : {
    "message" : [
      "Stream-stream join without equality predicate is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_1182" : {
    "message" : [
      "Column <ambiguousAttrs> are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set <config> to false to disable this check."
    ]
  },
  "_LEGACY_ERROR_TEMP_1183" : {
    "message" : [
      "Cannot use interval type in the table schema."
    ]
  },
  "_LEGACY_ERROR_TEMP_1184" : {
    "message" : [
      "Catalog <plugin> does not support <ability>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1185" : {
    "message" : [
      "<quoted> is not a valid <identifier> as it has more than 2 name parts."
    ]
  },
  "_LEGACY_ERROR_TEMP_1186" : {
    "message" : [
      "Multi-part identifier cannot be empty."
    ]
  },
  "_LEGACY_ERROR_TEMP_1187" : {
    "message" : [
      "Hive data source can only be used with tables, you can not <operation> files of Hive data source directly."
    ]
  },
  "_LEGACY_ERROR_TEMP_1188" : {
    "message" : [
      "There is a 'path' option set and <method>() is called with a path parameter. Either remove the path option, or call <method>() without the parameter. To ignore this check, set '<config>' to 'true'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1189" : {
    "message" : [
      "User specified schema not supported with `<operation>`."
    ]
  },
  "_LEGACY_ERROR_TEMP_1190" : {
    "message" : [
      "Temporary view <viewName> doesn't support streaming write."
    ]
  },
  "_LEGACY_ERROR_TEMP_1191" : {
    "message" : [
      "Streaming into views <viewName> is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_1192" : {
    "message" : [
      "The input source(<source>) is different from the table <tableName>'s data source provider(<provider>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1193" : {
    "message" : [
      "Table <tableName> doesn't support streaming write - <t>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1194" : {
    "message" : [
      "queryName must be specified for memory sink."
    ]
  },
  "_LEGACY_ERROR_TEMP_1195" : {
    "message" : [
      "'<source>' is not supported with continuous trigger."
    ]
  },
  "_LEGACY_ERROR_TEMP_1196" : {
    "message" : [
      "<columnType> column <columnName> not found in existing columns (<validColumnNames>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1197" : {
    "message" : [
      "'<operation>' does not support partitioning."
    ]
  },
  "_LEGACY_ERROR_TEMP_1198" : {
    "message" : [
      "Function '<unbound>' cannot process input: (<arguments>): <unsupported>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1199" : {
    "message" : [
      "Invalid bound function '<bound>: there are <argsLen> arguments but <inputTypesLen> parameters returned from 'inputTypes()'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1200" : {
    "message" : [
      "<name> is not supported for v2 tables."
    ]
  },
  "_LEGACY_ERROR_TEMP_1201" : {
    "message" : [
      "Cannot resolve column name \"<colName>\" among (<fieldNames>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1202" : {
    "message" : [
      "Cannot write to '<tableName>', too many data columns:",
      "Table columns: <tableColumns>",
      "Data columns: <dataColumns>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1203" : {
    "message" : [
      "Cannot write to '<tableName>', not enough data columns:",
      "Table columns: <tableColumns>",
      "Data columns: <dataColumns>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1204" : {
    "message" : [
      "Cannot write incompatible data to table '<tableName>':",
      "- <errors>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1205" : {
    "message" : [
      "Expected only partition pruning predicates: <nonPartitionPruningPredicates>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1206" : {
    "message" : [
      "<colType> column <colName> is not defined in table <tableName>, defined table columns are: <tableCols>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1207" : {
    "message" : [
      "The duration and time inputs to window must be an integer, long or string literal."
    ]
  },
  "_LEGACY_ERROR_TEMP_1209" : {
    "message" : [
      "Ambiguous reference to fields <fields>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1210" : {
    "message" : [
      "The second argument in <funcName> should be a boolean literal."
    ]
  },
  "_LEGACY_ERROR_TEMP_1211" : {
    "message" : [
      "Detected implicit cartesian product for <joinType> join between logical plans",
      "<leftPlan>",
      "and",
      "rightPlan",
      "Join condition is missing or trivial.",
      "Either: use the CROSS JOIN syntax to allow cartesian products between these relations, or: enable implicit cartesian products by setting the configuration variable spark.sql.crossJoin.enabled=true."
    ]
  },
  "_LEGACY_ERROR_TEMP_1212" : {
    "message" : [
      "Found conflicting attributes <conflictingAttrs> in the condition joining outer plan:",
      "<outerPlan>",
      "and subplan:",
      "<subplan>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1213" : {
    "message" : [
      "Window expression is empty in <expr>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1214" : {
    "message" : [
      "Found different window function type in <windowExpressions>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1215" : {
    "message" : [
      "char/varchar type can only be used in the table schema. You can set <config> to true, so that Spark treat them as string type as same as Spark 3.0 and earlier."
    ]
  },
  "_LEGACY_ERROR_TEMP_1216" : {
    "message" : [
      "The pattern '<pattern>' is invalid, <message>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1218" : {
    "message" : [
      "<tableIdentifier> should be converted to HadoopFsRelation."
    ]
  },
  "_LEGACY_ERROR_TEMP_1219" : {
    "message" : [
      "Hive metastore does not support altering database location"
    ]
  },
  "_LEGACY_ERROR_TEMP_1220" : {
    "message" : [
      "Hive <tableType> is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_1221" : {
    "message" : [
      "Hive 0.12 doesn't support creating permanent functions. Please use Hive 0.13 or higher."
    ]
  },
  "_LEGACY_ERROR_TEMP_1222" : {
    "message" : [
      "Unknown resource type: <resourceType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1223" : {
    "message" : [
      "Invalid field id '<field>' in day-time interval. Supported interval fields: <supportedIds>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1224" : {
    "message" : [
      "'interval <startFieldName> to <endFieldName>' is invalid."
    ]
  },
  "_LEGACY_ERROR_TEMP_1225" : {
    "message" : [
      "Invalid field id '<field>' in year-month interval. Supported interval fields: <supportedIds>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1226" : {
    "message" : [
      "The SQL config '<configName>' was removed in the version <version>. <comment>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1227" : {
    "message" : [
      "<msg><e1>",
      "Failed fallback parsing: <e2>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1228" : {
    "message" : [
      "Decimal scale (<scale>) cannot be greater than precision (<precision>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1229" : {
    "message" : [
      "<decimalType> can only support precision up to <precision>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1230" : {
    "message" : [
      "Negative scale is not allowed: <scale>. You can use <config>=true to enable legacy mode to allow it."
    ]
  },
  "_LEGACY_ERROR_TEMP_1231" : {
    "message" : [
      "<key> is not a valid partition column in table <tblName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1232" : {
    "message" : [
      "Partition spec is invalid. The spec (<specKeys>) must match the partition spec (<partitionColumnNames>) defined in table '<tableName>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1233" : {
    "message" : [
      "Found duplicate column(s) <colType>: <duplicateCol>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1234" : {
    "message" : [
      "Temporary view <tableIdent> is not cached for analyzing columns."
    ]
  },
  "_LEGACY_ERROR_TEMP_1235" : {
    "message" : [
      "Column <name> in table <tableIdent> is of type <dataType>, and Spark does not support statistics collection on this column type."
    ]
  },
  "_LEGACY_ERROR_TEMP_1236" : {
    "message" : [
      "ANALYZE TABLE is not supported on views."
    ]
  },
  "_LEGACY_ERROR_TEMP_1237" : {
    "message" : [
      "The list of partition columns with values in partition specification for table '<table>' in database '<database>' is not a prefix of the list of partition columns defined in the table schema. Expected a prefix of [<schemaColumns>], but got [<specColumns>]."
    ]
  },
  "_LEGACY_ERROR_TEMP_1239" : {
    "message" : [
      "Analyzing column statistics is not supported for column <name> of data type: <dataType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1241" : {
    "message" : [
      "CREATE-TABLE-AS-SELECT cannot create table with location to a non-empty directory <tablePath>. To allow overwriting the existing non-empty directory, set '<config>' to true."
    ]
  },
  "_LEGACY_ERROR_TEMP_1242" : {
    "message" : [
      "Undefined function: <rawName>. This function is neither a built-in/temporary function, nor a persistent function that is qualified as <fullName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1243" : {
    "message" : [
      "Undefined function: <rawName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1244" : {
    "message" : [
      "Attempted to unset non-existent property '<property>' in table '<table>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1245" : {
    "message" : [
      "ALTER TABLE CHANGE COLUMN is not supported for changing column '<originName>' with type '<originType>' to '<newName>' with type '<newType>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1246" : {
    "message" : [
      "Can't find column `<name>` given table data columns <fieldNames>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1247" : {
    "message" : [
      "Operation not allowed: ALTER TABLE SET [SERDE | SERDEPROPERTIES] for a specific partition is not supported for tables created with the datasource API."
    ]
  },
  "_LEGACY_ERROR_TEMP_1248" : {
    "message" : [
      "Operation not allowed: ALTER TABLE SET SERDE is not supported for tables created with the datasource API."
    ]
  },
  "_LEGACY_ERROR_TEMP_1249" : {
    "message" : [
      "Operation not allowed: <cmd> only works on partitioned tables: <tableIdentWithDB>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1250" : {
    "message" : [
      "<action> is not allowed on <tableName> since filesource partition management is disabled (spark.sql.hive.manageFilesourcePartitions = false)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1251" : {
    "message" : [
      "<action> is not allowed on <tableName> since its partition metadata is not stored in the Hive metastore. To import this information into the metastore, run `msck repair table <tableName>`."
    ]
  },
  "_LEGACY_ERROR_TEMP_1252" : {
    "message" : [
      "Cannot alter a view with ALTER TABLE. Please use ALTER VIEW instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1253" : {
    "message" : [
      "Cannot alter a table with ALTER VIEW. Please use ALTER TABLE instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1254" : {
    "message" : [
      "Cannot overwrite a path that is also being read from."
    ]
  },
  "_LEGACY_ERROR_TEMP_1255" : {
    "message" : [
      "Cannot drop built-in function '<functionName>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1256" : {
    "message" : [
      "Cannot refresh built-in function <functionName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1257" : {
    "message" : [
      "Cannot refresh temporary function <functionName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1259" : {
    "message" : [
      "ALTER ADD COLUMNS does not support views. You must drop and re-create the views for adding the new columns. Views: <table>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1260" : {
    "message" : [
      "ALTER ADD COLUMNS does not support datasource table with type <tableType>. You must drop and re-create the table for adding the new columns. Tables: <table>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1261" : {
    "message" : [
      "LOAD DATA is not supported for datasource tables: <tableIdentWithDB>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1262" : {
    "message" : [
      "LOAD DATA target table <tableIdentWithDB> is partitioned, but no partition spec is provided."
    ]
  },
  "_LEGACY_ERROR_TEMP_1263" : {
    "message" : [
      "LOAD DATA target table <tableIdentWithDB> is partitioned, but number of columns in provided partition spec (<partitionSize>) do not match number of partitioned columns in table (<targetTableSize>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1264" : {
    "message" : [
      "LOAD DATA target table <tableIdentWithDB> is not partitioned, but a partition spec was provided."
    ]
  },
  "_LEGACY_ERROR_TEMP_1265" : {
    "message" : [
      "LOAD DATA input path does not exist: <path>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1266" : {
    "message" : [
      "Operation not allowed: TRUNCATE TABLE on external tables: <tableIdentWithDB>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1267" : {
    "message" : [
      "Operation not allowed: TRUNCATE TABLE ... PARTITION is not supported for tables that are not partitioned: <tableIdentWithDB>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1268" : {
    "message" : [
      "Failed to truncate table <tableIdentWithDB> when removing data of the path: <path>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1269" : {
    "message" : [
      "SHOW PARTITIONS is not allowed on a table that is not partitioned: <tableIdentWithDB>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1270" : {
    "message" : [
      "SHOW CREATE TABLE is not supported on a temporary view: <table>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1271" : {
    "message" : [
      "Failed to execute SHOW CREATE TABLE against table <table>, which is created by Hive and uses the following unsupported feature(s)",
      "<unsupportedFeatures>",
      "Please use `SHOW CREATE TABLE <table> AS SERDE` to show Hive DDL instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1272" : {
    "message" : [
      "SHOW CREATE TABLE doesn't support transactional Hive table. Please use `SHOW CREATE TABLE <table> AS SERDE` to show Hive DDL instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1273" : {
    "message" : [
      "Failed to execute SHOW CREATE TABLE against table <table>, which is created by Hive and uses the following unsupported serde configuration",
      "<configs>",
      "Please use `SHOW CREATE TABLE <table> AS SERDE` to show Hive DDL instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1274" : {
    "message" : [
      "<table> is a Spark data source table. Use `SHOW CREATE TABLE` without `AS SERDE` instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1275" : {
    "message" : [
      "Failed to execute SHOW CREATE TABLE against table/view <table>, which is created by Hive and uses the following unsupported feature(s)",
      "<features>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1276" : {
    "message" : [
      "The logical plan that represents the view is not analyzed."
    ]
  },
  "_LEGACY_ERROR_TEMP_1277" : {
    "message" : [
      "The number of columns produced by the SELECT clause (num: `<analyzedPlanLength>`) does not match the number of column names specified by CREATE VIEW (num: `<userSpecifiedColumnsLength>`)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1278" : {
    "message" : [
      "<name> is not a view."
    ]
  },
  "_LEGACY_ERROR_TEMP_1280" : {
    "message" : [
      "It is not allowed to create a persisted view from the Dataset API."
    ]
  },
  "_LEGACY_ERROR_TEMP_1281" : {
    "message" : [
      "Recursive view <viewIdent> detected (cycle: <newPath>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1282" : {
    "message" : [
      "Not allowed to create a permanent view <name> without explicitly assigning an alias for expression <attrName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1283" : {
    "message" : [
      "Not allowed to create a permanent view <name> by referencing a temporary view <nameParts>. Please create a temp view instead by CREATE TEMP VIEW."
    ]
  },
  "_LEGACY_ERROR_TEMP_1284" : {
    "message" : [
      "Not allowed to create a permanent view <name> by referencing a temporary function `<funcName>`."
    ]
  },
  "_LEGACY_ERROR_TEMP_1285" : {
    "message" : [
      "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the",
      "referenced columns only include the internal corrupt record column",
      "(named _corrupt_record by default). For example:",
      "spark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()",
      "and spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().",
      "Instead, you can cache or save the parsed results and then send the same query.",
      "For example, val df = spark.read.schema(schema).csv(file).cache() and then",
      "df.filter($\"_corrupt_record\".isNotNull).count()."
    ]
  },
  "_LEGACY_ERROR_TEMP_1286" : {
    "message" : [
      "User-defined partition column <columnName> not found in the JDBC relation: <schema>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1287" : {
    "message" : [
      "Partition column type should be <numericType>, <dateType>, or <timestampType>, but <dataType> found."
    ]
  },
  "_LEGACY_ERROR_TEMP_1288" : {
    "message" : [
      "Table or view '<name>' already exists. SaveMode: ErrorIfExists."
    ]
  },
  "_LEGACY_ERROR_TEMP_1289" : {
    "message" : [
      "Column name \"<name>\" contains invalid character(s). Please use alias to rename it."
    ]
  },
  "_LEGACY_ERROR_TEMP_1290" : {
    "message" : [
      "Text data source supports only a single column, and you have <schemaSize> columns."
    ]
  },
  "_LEGACY_ERROR_TEMP_1291" : {
    "message" : [
      "Can't find required partition column <readField> in partition schema <partitionSchema>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1292" : {
    "message" : [
      "Temporary view '<tableIdent>' should not have specified a database."
    ]
  },
  "_LEGACY_ERROR_TEMP_1293" : {
    "message" : [
      "Hive data source can only be used with tables, you can't use it with CREATE TEMP VIEW USING."
    ]
  },
  "_LEGACY_ERROR_TEMP_1294" : {
    "message" : [
      "The timestamp provided for the '<strategy>' option is invalid. The expected format is 'YYYY-MM-DDTHH:mm:ss', but the provided timestamp: <timeString>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1295" : {
    "message" : [
      "Set a host to read from with option(\"host\", ...)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1296" : {
    "message" : [
      "Set a port to read from with option(\"port\", ...)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1297" : {
    "message" : [
      "IncludeTimestamp must be set to either \"true\" or \"false\"."
    ]
  },
  "_LEGACY_ERROR_TEMP_1298" : {
    "message" : [
      "checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"<config>\", ...)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1299" : {
    "message" : [
      "This query does not support recovering from checkpoint location. Delete <checkpointPath> to start over."
    ]
  },
  "_LEGACY_ERROR_TEMP_1300" : {
    "message" : [
      "Unable to find the column `<colName>` given [<actualColumns>]."
    ]
  },
  "_LEGACY_ERROR_TEMP_1301" : {
    "message" : [
      "Boundary start is not a valid integer: <start>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1302" : {
    "message" : [
      "Boundary end is not a valid integer: <end>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1304" : {
    "message" : [
      "Unexpected type <className> of the relation <tableName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1305" : {
    "message" : [
      "Unsupported TableChange <change> in JDBC catalog."
    ]
  },
  "_LEGACY_ERROR_TEMP_1306" : {
    "message" : [
      "There is a 'path' or 'paths' option set and load() is called with path parameters. Either remove the path option if it's the same as the path parameter, or add it to the load() parameter if you do want to read multiple paths. To ignore this check, set '<config>' to 'true'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1307" : {
    "message" : [
      "There is a 'path' option set and save() is called with a path parameter. Either remove the path option, or call save() without the parameter. To ignore this check, set '<config>' to 'true'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1308" : {
    "message" : [
      "TableProvider implementation <source> cannot be written with <createMode> mode, please use Append or Overwrite modes instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1309" : {
    "message" : [
      "insertInto() can't be used together with partitionBy(). Partition columns have already been defined for the table. It is not necessary to use partitionBy()."
    ]
  },
  "_LEGACY_ERROR_TEMP_1310" : {
    "message" : [
      "Couldn't find a catalog to handle the identifier <quote>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1311" : {
    "message" : [
      "sortBy must be used together with bucketBy."
    ]
  },
  "_LEGACY_ERROR_TEMP_1312" : {
    "message" : [
      "'<operation>' does not support bucketBy right now."
    ]
  },
  "_LEGACY_ERROR_TEMP_1313" : {
    "message" : [
      "'<operation>' does not support bucketBy and sortBy right now."
    ]
  },
  "_LEGACY_ERROR_TEMP_1315" : {
    "message" : [
      "Cannot overwrite table <tableName> that is also being read from."
    ]
  },
  "_LEGACY_ERROR_TEMP_1316" : {
    "message" : [
      "Invalid partition transformation: <expr>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1317" : {
    "message" : [
      "Cannot resolve column name \"<colName>\" among (<fieldsStr>)<extraMsg>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1318" : {
    "message" : [
      "Unable to parse '<delayThreshold>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_1319" : {
    "message" : [
      "Invalid join type in joinWith: <joinType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1320" : {
    "message" : [
      "Typed column <typedCol> that needs input type and schema cannot be passed in untyped `select` API. Use the typed `Dataset.select` API instead."
    ]
  },
  "_LEGACY_ERROR_TEMP_1321" : {
    "message" : [
      "Invalid view name: <viewName>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1322" : {
    "message" : [
      "Invalid number of buckets: bucket(<numBuckets>, <e>)."
    ]
  },
  "_LEGACY_ERROR_TEMP_1323" : {
    "message" : [
      "\"<colName>\" is not a numeric column. Aggregation function can only be applied on a numeric column."
    ]
  },
  "_LEGACY_ERROR_TEMP_1324" : {
    "message" : [
      "The pivot column <pivotColumn> has more than <maxValues> distinct values, this could indicate an error. If this was intended, set <config> to at least the number of distinct values of the pivot column."
    ]
  },
  "_LEGACY_ERROR_TEMP_1325" : {
    "message" : [
      "Cannot modify the value of a static config: <key>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1326" : {
    "message" : [
      "Cannot modify the value of a Spark config: <key>.",
      "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'"
    ]
  },
  "_LEGACY_ERROR_TEMP_1327" : {
    "message" : [
      "Command execution is not supported in runner <runner>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1328" : {
    "message" : [
      "Can not instantiate class <className>, please make sure it has public non argument constructor."
    ]
  },
  "_LEGACY_ERROR_TEMP_1329" : {
    "message" : [
      "Can not load class <className>, please make sure it is on the classpath."
    ]
  },
  "_LEGACY_ERROR_TEMP_1330" : {
    "message" : [
      "Class <className> doesn't implement interface UserDefinedAggregateFunction."
    ]
  },
  "_LEGACY_ERROR_TEMP_1331" : {
    "message" : [
      "Missing field <fieldName> in table <table> with schema:",
      "<schema>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1332" : {
    "message" : [
      "<errorMessage>"
    ]
  },
  "_LEGACY_ERROR_TEMP_1333" : {
    "message" : [
      "Invalid view text: <viewText>. The view <tableName> may have been tampered with."
    ]
  },
  "_LEGACY_ERROR_TEMP_1334" : {
    "message" : [
      "Cannot specify both version and timestamp when time travelling the table."
    ]
  },
  "_LEGACY_ERROR_TEMP_1335" : {
    "message" : [
      "<expr> is not a valid timestamp expression for time travel."
    ]
  },
  "_LEGACY_ERROR_TEMP_1336" : {
    "message" : [
      "Cannot time travel <target>."
    ]
  },
  "_LEGACY_ERROR_TEMP_1337" : {
    "message" : [
      "Table <tableName> does not support time travel."
    ]
  },
  "_LEGACY_ERROR_TEMP_1338" : {
    "message" : [
      "Sinks cannot request distribution and ordering in continuous execution mode"
    ]
  },
  "_LEGACY_ERROR_TEMP_1339" : {
    "message" : [
      "Failed to execute INSERT INTO command because the VALUES list contains a DEFAULT column reference as part of another expression; this is not allowed"
    ]
  },
  "_LEGACY_ERROR_TEMP_1340" : {
    "message" : [
      "Failed to execute UPDATE command because the SET list contains a DEFAULT column reference as part of another expression; this is not allowed."
    ]
  },
  "_LEGACY_ERROR_TEMP_1341" : {
    "message" : [
      "Failed to execute UPDATE command because the WHERE clause contains a DEFAULT column reference; this is not allowed."
    ]
  },
  "_LEGACY_ERROR_TEMP_1342" : {
    "message" : [
      "Failed to execute MERGE command because the WHERE clause contains a DEFAULT column reference; this is not allowed."
    ]
  },
  "_LEGACY_ERROR_TEMP_1343" : {
    "message" : [
      "Failed to execute MERGE INTO command because one of its INSERT or UPDATE assignments contains a DEFAULT column reference as part of another expression; this is not allowed."
    ]
  },
  "_LEGACY_ERROR_TEMP_1344" : {
    "message" : [
      "Invalid DEFAULT value for column <fieldName>: <defaultValue> fails to parse as a valid literal value."
    ]
  },
  "_LEGACY_ERROR_TEMP_1345" : {
    "message" : [
      "Failed to execute <statementType> command because DEFAULT values are not supported for target data source with table provider: \"<dataSource>\"."
    ]
  },
  "_LEGACY_ERROR_TEMP_1346" : {
    "message" : [
      "Failed to execute <statementType> command because DEFAULT values are not supported when adding new columns to previously existing target data source with table provider: \"<dataSource>\"."
    ]
  },
  "_LEGACY_ERROR_TEMP_1347" : {
    "message" : [
      "Failed to execute command because subquery expressions are not allowed in DEFAULT values."
    ]
  },
  "_LEGACY_ERROR_TEMP_2000" : {
    "message" : [
      "<message>. If necessary set <ansiConfig> to false to bypass this error."
    ]
  },
  "_LEGACY_ERROR_TEMP_2003" : {
    "message" : [
      "Unsuccessful try to zip maps with <size> unique keys due to exceeding the array size limit <maxRoundedArrayLength>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2004" : {
    "message" : [
      "no default for type <dataType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2005" : {
    "message" : [
      "Type <dataType> does not support ordered operations"
    ]
  },
  "_LEGACY_ERROR_TEMP_2006" : {
    "message" : [
      "The specified group index cannot be less than zero"
    ]
  },
  "_LEGACY_ERROR_TEMP_2007" : {
    "message" : [
      "Regex group count is <groupCount>, but the specified group index is <groupIndex>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2008" : {
    "message" : [
      "Find an invalid url string <url>. If necessary set <ansiConfig> to false to bypass this error."
    ]
  },
  "_LEGACY_ERROR_TEMP_2009" : {
    "message" : [
      "dataType"
    ]
  },
  "_LEGACY_ERROR_TEMP_2010" : {
    "message" : [
      "Window Functions do not support merging."
    ]
  },
  "_LEGACY_ERROR_TEMP_2011" : {
    "message" : [
      "Unexpected data type <dataType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2013" : {
    "message" : [
      "Negative values found in <frequencyExpression>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2014" : {
    "message" : [
      "<funcName> is not matched at addNewFunction"
    ]
  },
  "_LEGACY_ERROR_TEMP_2015" : {
    "message" : [
      "Cannot generate <codeType> code for incomparable type: <dataType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2016" : {
    "message" : [
      "Can not interpolate <arg> into code block."
    ]
  },
  "_LEGACY_ERROR_TEMP_2017" : {
    "message" : [
      "not resolved"
    ]
  },
  "_LEGACY_ERROR_TEMP_2018" : {
    "message" : [
      "class `<cls>` is not supported by `MapObjects` as resulting collection."
    ]
  },
  "_LEGACY_ERROR_TEMP_2019" : {
    "message" : [
      "Cannot use null as map key!"
    ]
  },
  "_LEGACY_ERROR_TEMP_2020" : {
    "message" : [
      "Couldn't find a valid constructor on <cls>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2021" : {
    "message" : [
      "Couldn't find a primary constructor on <cls>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2022" : {
    "message" : [
      "Unsupported natural join type <joinType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2023" : {
    "message" : [
      "Unresolved encoder expected, but <attr> was found."
    ]
  },
  "_LEGACY_ERROR_TEMP_2024" : {
    "message" : [
      "Only expression encoders are supported for now."
    ]
  },
  "_LEGACY_ERROR_TEMP_2025" : {
    "message" : [
      "<className> must override either <m1> or <m2>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2026" : {
    "message" : [
      "Failed to convert value <value> (class of <cls>) with the type of <dataType> to JSON."
    ]
  },
  "_LEGACY_ERROR_TEMP_2027" : {
    "message" : [
      "Unexpected operator <op> in correlated subquery<pos>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2028" : {
    "message" : [
      "This line should be unreachable<err>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2029" : {
    "message" : [
      "Not supported rounding mode: <roundMode>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2030" : {
    "message" : [
      "Can not handle nested schema yet...  plan <plan>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2031" : {
    "message" : [
      "The input external row cannot be null."
    ]
  },
  "_LEGACY_ERROR_TEMP_2032" : {
    "message" : [
      "<fieldCannotBeNullMsg>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2033" : {
    "message" : [
      "Unable to create database <name> as failed to create its directory <locationUri>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2034" : {
    "message" : [
      "Unable to drop database <name> as failed to delete its directory <locationUri>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2035" : {
    "message" : [
      "Unable to create table <table> as failed to create its directory <defaultTableLocation>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2036" : {
    "message" : [
      "Unable to delete partition path <partitionPath>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2037" : {
    "message" : [
      "Unable to drop table <table> as failed to delete its directory <dir>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2038" : {
    "message" : [
      "Unable to rename table <oldName> to <newName> as failed to rename its directory <oldDir>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2039" : {
    "message" : [
      "Unable to create partition path <partitionPath>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2040" : {
    "message" : [
      "Unable to rename partition path <oldPartPath>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2041" : {
    "message" : [
      "<methodName> is not implemented"
    ]
  },
  "_LEGACY_ERROR_TEMP_2042" : {
    "message" : [
      "<message>. If necessary set <ansiConfig> to false to bypass this error."
    ]
  },
  "_LEGACY_ERROR_TEMP_2043" : {
    "message" : [
      "- <sqlValue> caused overflow"
    ]
  },
  "_LEGACY_ERROR_TEMP_2044" : {
    "message" : [
      "<sqlValue1> <symbol> <sqlValue2> caused overflow"
    ]
  },
  "_LEGACY_ERROR_TEMP_2045" : {
    "message" : [
      "Unsupported table change: <message>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2046" : {
    "message" : [
      "[BUG] Not a DataSourceRDDPartition: <split>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2047" : {
    "message" : [
      "'path' is not specified"
    ]
  },
  "_LEGACY_ERROR_TEMP_2048" : {
    "message" : [
      "Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it."
    ]
  },
  "_LEGACY_ERROR_TEMP_2049" : {
    "message" : [
      "Data source <className> does not support streamed <operator>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2050" : {
    "message" : [
      "Expected exactly one path to be specified, but got: <paths>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2051" : {
    "message" : [
      "Failed to find data source: <provider>. Please find packages at https://spark.apache.org/third-party-projects.html"
    ]
  },
  "_LEGACY_ERROR_TEMP_2052" : {
    "message" : [
      "<className> was removed in Spark 2.0. Please check if your library is compatible with Spark 2.0"
    ]
  },
  "_LEGACY_ERROR_TEMP_2053" : {
    "message" : [
      "buildReader is not supported for <format>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2054" : {
    "message" : [
      "Task failed while writing rows."
    ]
  },
  "_LEGACY_ERROR_TEMP_2055" : {
    "message" : [
      "<message>",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved."
    ]
  },
  "_LEGACY_ERROR_TEMP_2056" : {
    "message" : [
      "Unable to clear output directory <staticPrefixPath> prior to writing to it"
    ]
  },
  "_LEGACY_ERROR_TEMP_2057" : {
    "message" : [
      "Unable to clear partition directory <path> prior to writing to it"
    ]
  },
  "_LEGACY_ERROR_TEMP_2058" : {
    "message" : [
      "Failed to cast value `<value>` to `<dataType>` for partition column `<columnName>`"
    ]
  },
  "_LEGACY_ERROR_TEMP_2059" : {
    "message" : [
      "End of stream"
    ]
  },
  "_LEGACY_ERROR_TEMP_2060" : {
    "message" : [
      "The fallback v1 relation reports inconsistent schema:",
      "Schema of v2 scan: <v2Schema>",
      "Schema of v1 relation: <v1Schema>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2061" : {
    "message" : [
      "No records should be returned from EmptyDataReader"
    ]
  },
  "_LEGACY_ERROR_TEMP_2062" : {
    "message" : [
      "<message>",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by recreating the Dataset/DataFrame involved."
    ]
  },
  "_LEGACY_ERROR_TEMP_2063" : {
    "message" : [
      "Parquet column cannot be converted in file <filePath>. Column: <column>, Expected: <logicalType>, Found: <physicalType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2064" : {
    "message" : [
      "Encountered error while reading file <path>. Details:"
    ]
  },
  "_LEGACY_ERROR_TEMP_2065" : {
    "message" : [
      "Cannot create columnar reader."
    ]
  },
  "_LEGACY_ERROR_TEMP_2066" : {
    "message" : [
      "Invalid namespace name: <namespace>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2067" : {
    "message" : [
      "Unsupported partition transform: <transform>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2068" : {
    "message" : [
      "Missing database location"
    ]
  },
  "_LEGACY_ERROR_TEMP_2069" : {
    "message" : [
      "Cannot remove reserved property: <property>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2070" : {
    "message" : [
      "Writing job failed."
    ]
  },
  "_LEGACY_ERROR_TEMP_2071" : {
    "message" : [
      "Commit denied for partition <partId> (task <taskId>, attempt <attemptId>, stage <stageId>.<stageAttempt>)"
    ]
  },
  "_LEGACY_ERROR_TEMP_2072" : {
    "message" : [
      "Table implementation does not support writes: <ident>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2073" : {
    "message" : [
      "Cannot create JDBC table with partition"
    ]
  },
  "_LEGACY_ERROR_TEMP_2074" : {
    "message" : [
      "user-specified schema"
    ]
  },
  "_LEGACY_ERROR_TEMP_2075" : {
    "message" : [
      "Write is not supported for binary file data source"
    ]
  },
  "_LEGACY_ERROR_TEMP_2076" : {
    "message" : [
      "The length of <path> is <len>, which exceeds the max length allowed: <maxLength>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2077" : {
    "message" : [
      "Unsupported field name: <fieldName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2078" : {
    "message" : [
      "Both '<jdbcTableName>' and '<jdbcQueryString>' can not be specified at the same time."
    ]
  },
  "_LEGACY_ERROR_TEMP_2079" : {
    "message" : [
      "Option '<jdbcTableName>' or '<jdbcQueryString>' is required."
    ]
  },
  "_LEGACY_ERROR_TEMP_2080" : {
    "message" : [
      "Option `<optionName>` can not be empty."
    ]
  },
  "_LEGACY_ERROR_TEMP_2081" : {
    "message" : [
      "Invalid value `<value>` for parameter `<jdbcTxnIsolationLevel>`. This can be `NONE`, `READ_UNCOMMITTED`, `READ_COMMITTED`, `REPEATABLE_READ` or `SERIALIZABLE`."
    ]
  },
  "_LEGACY_ERROR_TEMP_2082" : {
    "message" : [
      "Can't get JDBC type for <catalogString>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2083" : {
    "message" : [
      "Unsupported type <content>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2084" : {
    "message" : [
      "Unsupported array element type <catalogString> based on binary"
    ]
  },
  "_LEGACY_ERROR_TEMP_2085" : {
    "message" : [
      "Nested arrays unsupported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2086" : {
    "message" : [
      "Can't translate non-null value for field <pos>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2087" : {
    "message" : [
      "Invalid value `<n>` for parameter `<jdbcNumPartitions>` in table writing via JDBC. The minimum value is 1."
    ]
  },
  "_LEGACY_ERROR_TEMP_2088" : {
    "message" : [
      "<dataType> is not supported yet."
    ]
  },
  "_LEGACY_ERROR_TEMP_2089" : {
    "message" : [
      "DataType: <catalogString>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2090" : {
    "message" : [
      "The input filter of <owner> should be fully convertible."
    ]
  },
  "_LEGACY_ERROR_TEMP_2091" : {
    "message" : [
      "Could not read footer for file: <file>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2092" : {
    "message" : [
      "Could not read footer for file: <file>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2093" : {
    "message" : [
      "Found duplicate field(s) \"<requiredFieldName>\": <matchedOrcFields> in case-insensitive mode"
    ]
  },
  "_LEGACY_ERROR_TEMP_2094" : {
    "message" : [
      "Found duplicate field(s) \"<requiredId>\": <matchedFields> in id mapping mode"
    ]
  },
  "_LEGACY_ERROR_TEMP_2095" : {
    "message" : [
      "Failed to merge incompatible schemas <left> and <right>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2096" : {
    "message" : [
      "<ddl> is not supported temporarily."
    ]
  },
  "_LEGACY_ERROR_TEMP_2097" : {
    "message" : [
      "Could not execute broadcast in <timeout> secs. You can increase the timeout for broadcasts via <broadcastTimeout> or disable broadcast join by setting <autoBroadcastJoinThreshold> to -1"
    ]
  },
  "_LEGACY_ERROR_TEMP_2098" : {
    "message" : [
      "Could not compare cost with <cost>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2099" : {
    "message" : [
      "Unsupported data type: <dt>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2100" : {
    "message" : [
      "not support type: <dataType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2101" : {
    "message" : [
      "Not support non-primitive type now"
    ]
  },
  "_LEGACY_ERROR_TEMP_2102" : {
    "message" : [
      "Unsupported type: <catalogString>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2103" : {
    "message" : [
      "Dictionary encoding should not be used because of dictionary overflow."
    ]
  },
  "_LEGACY_ERROR_TEMP_2104" : {
    "message" : [
      "End of the iterator"
    ]
  },
  "_LEGACY_ERROR_TEMP_2105" : {
    "message" : [
      "Could not allocate memory to grow BytesToBytesMap"
    ]
  },
  "_LEGACY_ERROR_TEMP_2106" : {
    "message" : [
      "Can't acquire <size> bytes memory to build hash relation, got <got> bytes"
    ]
  },
  "_LEGACY_ERROR_TEMP_2107" : {
    "message" : [
      "There is not enough memory to build hash map"
    ]
  },
  "_LEGACY_ERROR_TEMP_2108" : {
    "message" : [
      "Does not support row that is larger than 256M"
    ]
  },
  "_LEGACY_ERROR_TEMP_2109" : {
    "message" : [
      "Cannot build HashedRelation with more than 1/3 billions unique keys"
    ]
  },
  "_LEGACY_ERROR_TEMP_2110" : {
    "message" : [
      "Can not build a HashedRelation that is larger than 8G"
    ]
  },
  "_LEGACY_ERROR_TEMP_2111" : {
    "message" : [
      "failed to push a row into <rowQueue>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2112" : {
    "message" : [
      "Unexpected window function frame <frame>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2113" : {
    "message" : [
      "Unable to parse <stats> as a percentile"
    ]
  },
  "_LEGACY_ERROR_TEMP_2114" : {
    "message" : [
      "<stats> is not a recognised statistic"
    ]
  },
  "_LEGACY_ERROR_TEMP_2115" : {
    "message" : [
      "Unknown column: <unknownColumn>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2116" : {
    "message" : [
      "Unexpected: <o>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2120" : {
    "message" : [
      "Do not support array of type <clazz>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2121" : {
    "message" : [
      "Do not support type <clazz>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2122" : {
    "message" : [
      "Failed parsing <simpleString>: <raw>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2123" : {
    "message" : [
      "Failed to merge fields '<leftName>' and '<rightName>'. <message>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2124" : {
    "message" : [
      "Failed to merge decimal types with incompatible scale <leftScale> and <rightScale>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2125" : {
    "message" : [
      "Failed to merge incompatible data types <leftCatalogString> and <rightCatalogString>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2126" : {
    "message" : [
      "Unsuccessful attempt to build maps with <size> elements due to exceeding the map size limit <maxRoundedArrayLength>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2127" : {
    "message" : [
      "Duplicate map key <key> was found, please check the input data. If you want to remove the duplicated keys, you can set <mapKeyDedupPolicy> to <lastWin> so that the key inserted at last takes precedence."
    ]
  },
  "_LEGACY_ERROR_TEMP_2128" : {
    "message" : [
      "The key array and value array of MapData must have the same length."
    ]
  },
  "_LEGACY_ERROR_TEMP_2129" : {
    "message" : [
      "Conflict found: Field <field> <actual> differs from <field> <expected> derived from <candidate>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2130" : {
    "message" : [
      "Fail to recognize '<pattern>' pattern in the DateTimeFormatter. You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html"
    ]
  },
  "_LEGACY_ERROR_TEMP_2131" : {
    "message" : [
      "Exception when registering StreamingQueryListener"
    ]
  },
  "_LEGACY_ERROR_TEMP_2132" : {
    "message" : [
      "Parsing JSON arrays as structs is forbidden."
    ]
  },
  "_LEGACY_ERROR_TEMP_2133" : {
    "message" : [
      "Cannot parse field name <fieldName>, field value <fieldValue>, [<token>] as target spark data type [<dataType>]."
    ]
  },
  "_LEGACY_ERROR_TEMP_2134" : {
    "message" : [
      "Cannot parse field value <value> for pattern <pattern> as target spark data type [<dataType>]."
    ]
  },
  "_LEGACY_ERROR_TEMP_2135" : {
    "message" : [
      "Failed to parse an empty string for data type <dataType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2136" : {
    "message" : [
      "Failed to parse field name <fieldName>, field value <fieldValue>, [<token>] to target spark data type [<dataType>]."
    ]
  },
  "_LEGACY_ERROR_TEMP_2137" : {
    "message" : [
      "Root converter returned null"
    ]
  },
  "_LEGACY_ERROR_TEMP_2138" : {
    "message" : [
      "Cannot have circular references in bean class, but got the circular reference of class <clazz>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2139" : {
    "message" : [
      "cannot have circular references in class, but got the circular reference of class <t>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2140" : {
    "message" : [
      "`<fieldName>` is not a valid identifier of Java and cannot be used as field name",
      "<walkedTypePath>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2141" : {
    "message" : [
      "No Encoder found for <tpe>",
      "<walkedTypePath>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2142" : {
    "message" : [
      "Attributes for type <schema> is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2143" : {
    "message" : [
      "Schema for type <tpe> is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2144" : {
    "message" : [
      "Unable to find constructor for <tpe>. This could happen if <tpe> is an interface, or a trait without companion object constructor."
    ]
  },
  "_LEGACY_ERROR_TEMP_2145" : {
    "message" : [
      "<paramName> cannot be more than one character"
    ]
  },
  "_LEGACY_ERROR_TEMP_2146" : {
    "message" : [
      "<paramName> should be an integer. Found <value>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2147" : {
    "message" : [
      "<paramName> flag can be true or false"
    ]
  },
  "_LEGACY_ERROR_TEMP_2148" : {
    "message" : [
      "null value found but field <name> is not nullable."
    ]
  },
  "_LEGACY_ERROR_TEMP_2149" : {
    "message" : [
      "Malformed CSV record"
    ]
  },
  "_LEGACY_ERROR_TEMP_2150" : {
    "message" : [
      "Due to Scala's limited support of tuple, tuple with more than 22 elements are not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_2151" : {
    "message" : [
      "Error while decoding: <e>",
      "<expressions>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2152" : {
    "message" : [
      "Error while encoding: <e>",
      "<expressions>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2153" : {
    "message" : [
      "class <clsName> has unexpected serializer: <objSerializer>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2154" : {
    "message" : [
      "Failed to get outer pointer for <innerCls>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2155" : {
    "message" : [
      "<userClass> is not annotated with SQLUserDefinedType nor registered with UDTRegistration.}"
    ]
  },
  "_LEGACY_ERROR_TEMP_2156" : {
    "message" : [
      "The size function doesn't support the operand type <dataType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2157" : {
    "message" : [
      "Unexpected value for start in function <prettyName>: SQL array indices start at 1."
    ]
  },
  "_LEGACY_ERROR_TEMP_2158" : {
    "message" : [
      "Unexpected value for length in function <prettyName>: length must be greater than or equal to 0."
    ]
  },
  "_LEGACY_ERROR_TEMP_2159" : {
    "message" : [
      "Unsuccessful try to concat arrays with <numberOfElements> elements due to exceeding the array size limit <maxRoundedArrayLength>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2160" : {
    "message" : [
      "Unsuccessful try to flatten an array of arrays with <numberOfElements> elements due to exceeding the array size limit <maxRoundedArrayLength>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2161" : {
    "message" : [
      "Unsuccessful try to create array with <count> elements due to exceeding the array size limit <maxRoundedArrayLength>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2162" : {
    "message" : [
      "Unsuccessful try to union arrays with <length> elements due to exceeding the array size limit <maxRoundedArrayLength>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2163" : {
    "message" : [
      "Initial type <dataType> must be a <target>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2164" : {
    "message" : [
      "Initial type <dataType> must be an <arrayType>, a <structType> or a <mapType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2165" : {
    "message" : [
      "Malformed records are detected in schema inference. Parse Mode: <failFastMode>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2166" : {
    "message" : [
      "Malformed JSON"
    ]
  },
  "_LEGACY_ERROR_TEMP_2167" : {
    "message" : [
      "Malformed records are detected in schema inference. Parse Mode: <failFastMode>. Reasons: Failed to infer a common schema. Struct types are expected, but `<dataType>` was found."
    ]
  },
  "_LEGACY_ERROR_TEMP_2168" : {
    "message" : [
      "Decorrelate inner query through <plan> is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_2169" : {
    "message" : [
      "This method should not be called in the analyzer"
    ]
  },
  "_LEGACY_ERROR_TEMP_2170" : {
    "message" : [
      "Cannot safely merge SERDEPROPERTIES:",
      "<props1>",
      "<props2>",
      "The conflict keys: <conflictKeys>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2171" : {
    "message" : [
      "Not supported pair: <r1>, <r2> at <function>()"
    ]
  },
  "_LEGACY_ERROR_TEMP_2172" : {
    "message" : [
      "Once strategy's idempotence is broken for batch <batchName>",
      "<plan>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2173" : {
    "message" : [
      "The structural integrity of the input plan is broken in <className>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2174" : {
    "message" : [
      "After applying rule <ruleName> in batch <batchName>, the structural integrity of the plan is broken."
    ]
  },
  "_LEGACY_ERROR_TEMP_2175" : {
    "message" : [
      "Rule id not found for <ruleName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2176" : {
    "message" : [
      "Cannot create array with <numElements> elements of data due to exceeding the limit <maxRoundedArrayLength> elements for ArrayData. <additionalErrorMessage>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2177" : {
    "message" : [
      "Malformed records are detected in record parsing. Parse Mode: <failFastMode>. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'."
    ]
  },
  "_LEGACY_ERROR_TEMP_2178" : {
    "message" : [
      "Remote operations not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2179" : {
    "message" : [
      "HiveServer2 Kerberos principal or keytab is not correctly configured"
    ]
  },
  "_LEGACY_ERROR_TEMP_2180" : {
    "message" : [
      "Parent SparkUI to attach this tab to not found!"
    ]
  },
  "_LEGACY_ERROR_TEMP_2181" : {
    "message" : [
      "inferSchema is not supported for hive data source."
    ]
  },
  "_LEGACY_ERROR_TEMP_2182" : {
    "message" : [
      "Requested partitioning does not match the <tableIdentifier> table:",
      "Requested partitions: <partitionKeys>",
      "Table partitions: <partitionColumnNames>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2183" : {
    "message" : [
      "Dynamic partition key <key> is not among written partition paths."
    ]
  },
  "_LEGACY_ERROR_TEMP_2184" : {
    "message" : [
      "Cannot remove partition directory '<partitionPath>'"
    ]
  },
  "_LEGACY_ERROR_TEMP_2185" : {
    "message" : [
      "Cannot create staging directory: <message>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2186" : {
    "message" : [
      "The SerDe interface removed since Hive 2.3(HIVE-15167). Please migrate your custom SerDes to Hive 2.3. See HIVE-15167 for more details."
    ]
  },
  "_LEGACY_ERROR_TEMP_2187" : {
    "message" : [
      "<message>, db: <dbName>, table: <tableName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2188" : {
    "message" : [
      "Cannot recognize hive type string: <fieldType>, column: <fieldName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2189" : {
    "message" : [
      "Hive 2.2 and lower versions don't support getTablesByType. Please use Hive 2.3 or higher version."
    ]
  },
  "_LEGACY_ERROR_TEMP_2190" : {
    "message" : [
      "DROP TABLE ... PURGE"
    ]
  },
  "_LEGACY_ERROR_TEMP_2191" : {
    "message" : [
      "ALTER TABLE ... DROP PARTITION ... PURGE"
    ]
  },
  "_LEGACY_ERROR_TEMP_2192" : {
    "message" : [
      "Partition filter cannot have both `\"` and `'` characters"
    ]
  },
  "_LEGACY_ERROR_TEMP_2193" : {
    "message" : [
      "Caught Hive MetaException attempting to get partition metadata by filter from Hive. You can set the Spark configuration setting <hiveMetastorePartitionPruningFallbackOnException> to true to work around this problem, however this will result in degraded performance. Please report a bug: https://issues.apache.org/jira/browse/SPARK"
    ]
  },
  "_LEGACY_ERROR_TEMP_2194" : {
    "message" : [
      "Unsupported Hive Metastore version <version>. Please set <key> with a valid version."
    ]
  },
  "_LEGACY_ERROR_TEMP_2195" : {
    "message" : [
      "<cnf> when creating Hive client using classpath: <execJars> Please make sure that jars for your version of hive and hadoop are included in the paths passed to <key>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2196" : {
    "message" : [
      "Unable to fetch tables of db <dbName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2197" : {
    "message" : [
      "LOCATION clause illegal for view partition"
    ]
  },
  "_LEGACY_ERROR_TEMP_2198" : {
    "message" : [
      "Failed to rename as <dstPath> already exists"
    ]
  },
  "_LEGACY_ERROR_TEMP_2199" : {
    "message" : [
      "Failed to rename temp file <srcPath> to <dstPath> as rename returned false"
    ]
  },
  "_LEGACY_ERROR_TEMP_2200" : {
    "message" : [
      "Error: we detected a possible problem with the location of your \"_spark_metadata\"",
      "directory and you likely need to move it before restarting this query.",
      "",
      "Earlier version of Spark incorrectly escaped paths when writing out the",
      "\"_spark_metadata\" directory for structured streaming. While this was corrected in",
      "Spark 3.0, it appears that your query was started using an earlier version that",
      "",
      "Correct \"_spark_metadata\" Directory: <metadataPath>",
      "Incorrect \"_spark_metadata\" Directory: <legacyMetadataPath>",
      "",
      "Please move the data from the incorrect directory to the correct one, delete the",
      "incorrect directory, and then restart this query. If you believe you are receiving",
      "this message in error, you can disable it with the SQL conf",
      "<StreamingCheckpointEscaptedPathCheckEnabled>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2201" : {
    "message" : [
      "Partition column <col> not found in schema <schema>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2203" : {
    "message" : [
      "Cannot set timeout duration without enabling processing time timeout in [map|flatMap]GroupsWithState"
    ]
  },
  "_LEGACY_ERROR_TEMP_2204" : {
    "message" : [
      "Cannot get event time watermark timestamp without setting watermark before [map|flatMap]GroupsWithState"
    ]
  },
  "_LEGACY_ERROR_TEMP_2205" : {
    "message" : [
      "Cannot set timeout timestamp without enabling event time timeout in [map|flatMapGroupsWithState"
    ]
  },
  "_LEGACY_ERROR_TEMP_2206" : {
    "message" : [
      "Unable to find batch <batchMetadataFile>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2207" : {
    "message" : [
      "Multiple streaming queries are concurrently using <path>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2208" : {
    "message" : [
      "<commitProtocol> does not support adding files with an absolute path"
    ]
  },
  "_LEGACY_ERROR_TEMP_2209" : {
    "message" : [
      "Data source <srcName> does not support microbatch processing.",
      "",
      "Either the data source is disabled at",
      "SQLConf.get.DISABLED_V2_STREAMING_MICROBATCH_READERS.key (The disabled sources",
      "are [<disabledSources>]) or the table <table> does not have MICRO_BATCH_READ",
      "capability. Meanwhile, the fallback, data source v1, is not available.\""
    ]
  },
  "_LEGACY_ERROR_TEMP_2210" : {
    "message" : [
      "StreamingRelationExec cannot be executed"
    ]
  },
  "_LEGACY_ERROR_TEMP_2211" : {
    "message" : [
      "Invalid output mode: <outputMode>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2212" : {
    "message" : [
      "Invalid catalog name: <name>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2214" : {
    "message" : [
      "Plugin class for catalog '<name>' does not implement CatalogPlugin: <pluginClassName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2215" : {
    "message" : [
      "Cannot find catalog plugin class for catalog '<name>': <pluginClassName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2216" : {
    "message" : [
      "Failed to find public no-arg constructor for catalog '<name>': <pluginClassName>)"
    ]
  },
  "_LEGACY_ERROR_TEMP_2217" : {
    "message" : [
      "Failed to call public no-arg constructor for catalog '<name>': <pluginClassName>)"
    ]
  },
  "_LEGACY_ERROR_TEMP_2218" : {
    "message" : [
      "Cannot instantiate abstract catalog plugin class for catalog '<name>': <pluginClassName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2219" : {
    "message" : [
      "Failed during instantiating constructor for catalog '<name>': <pluginClassName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2220" : {
    "message" : [
      ""
    ]
  },
  "_LEGACY_ERROR_TEMP_2222" : {
    "message" : [
      "Cannot mutate ReadOnlySQLConf."
    ]
  },
  "_LEGACY_ERROR_TEMP_2223" : {
    "message" : [
      "Cannot clone/copy ReadOnlySQLConf."
    ]
  },
  "_LEGACY_ERROR_TEMP_2224" : {
    "message" : [
      "Cannot get SQLConf inside scheduler event loop thread."
    ]
  },
  "_LEGACY_ERROR_TEMP_2225" : {
    "message" : [
      ""
    ]
  },
  "_LEGACY_ERROR_TEMP_2226" : {
    "message" : [
      "null literals can't be casted to <name>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2227" : {
    "message" : [
      "<name> is not an UserDefinedType. Please make sure registering an UserDefinedType for <userClass>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2228" : {
    "message" : [
      "Can not load in UserDefinedType <name> for user class <userClass>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2229" : {
    "message" : [
      "<name> is not a public class. Only public classes are supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_2230" : {
    "message" : [
      "Primitive types are not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_2231" : {
    "message" : [
      "fieldIndex on a Row without schema is undefined."
    ]
  },
  "_LEGACY_ERROR_TEMP_2232" : {
    "message" : [
      "Value at index <index> is null"
    ]
  },
  "_LEGACY_ERROR_TEMP_2233" : {
    "message" : [
      "Only Data Sources providing FileFormat are supported: <providingClass>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2234" : {
    "message" : [
      "Failed to set original ACL <aclEntries> back to the created path: <path>. Exception: <message>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2235" : {
    "message" : [
      "Multiple failures in stage materialization."
    ]
  },
  "_LEGACY_ERROR_TEMP_2236" : {
    "message" : [
      "Unrecognized compression scheme type ID: <typeId>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2237" : {
    "message" : [
      "<className>.getParentLogger is not yet implemented."
    ]
  },
  "_LEGACY_ERROR_TEMP_2238" : {
    "message" : [
      "Unable to create Parquet converter for <typeName> whose Parquet type is <parquetType> without decimal metadata. Please read this column/field as Spark BINARY type."
    ]
  },
  "_LEGACY_ERROR_TEMP_2239" : {
    "message" : [
      "Unable to create Parquet converter for decimal type <t> whose Parquet type is <parquetType>.  Parquet DECIMAL type can only be backed by INT32, INT64, FIXED_LEN_BYTE_ARRAY, or BINARY."
    ]
  },
  "_LEGACY_ERROR_TEMP_2240" : {
    "message" : [
      "Unable to create Parquet converter for data type <t> whose Parquet type is <parquetType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2241" : {
    "message" : [
      "Nonatomic partition table <tableName> can not add multiple partitions."
    ]
  },
  "_LEGACY_ERROR_TEMP_2242" : {
    "message" : [
      "<provider> source does not support user-specified schema."
    ]
  },
  "_LEGACY_ERROR_TEMP_2243" : {
    "message" : [
      "Nonatomic partition table <tableName> can not drop multiple partitions."
    ]
  },
  "_LEGACY_ERROR_TEMP_2244" : {
    "message" : [
      "The table <tableName> does not support truncation of multiple partition."
    ]
  },
  "_LEGACY_ERROR_TEMP_2245" : {
    "message" : [
      "Table does not support overwrite by expression: <table>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2246" : {
    "message" : [
      "Table does not support dynamic partition overwrite: <table>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2247" : {
    "message" : [
      "Failed merging schema:",
      "<schema>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2248" : {
    "message" : [
      "Cannot broadcast the table over <maxBroadcastTableRows> rows: <numRows> rows"
    ]
  },
  "_LEGACY_ERROR_TEMP_2249" : {
    "message" : [
      "Cannot broadcast the table that is larger than <maxBroadcastTableBytes>GB: <dataSize> GB"
    ]
  },
  "_LEGACY_ERROR_TEMP_2250" : {
    "message" : [
      "Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting <autoBroadcastjoinThreshold> to -1 or increase the spark driver memory by setting <driverMemory> to a higher value<analyzeTblMsg>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2251" : {
    "message" : [
      "<execName> does not support the execute() code path."
    ]
  },
  "_LEGACY_ERROR_TEMP_2252" : {
    "message" : [
      "Cannot merge <className> with <otherClass>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2253" : {
    "message" : [
      "Data source <sourceName> does not support continuous processing."
    ]
  },
  "_LEGACY_ERROR_TEMP_2254" : {
    "message" : [
      "Data read failed"
    ]
  },
  "_LEGACY_ERROR_TEMP_2255" : {
    "message" : [
      "Epoch marker generation failed"
    ]
  },
  "_LEGACY_ERROR_TEMP_2256" : {
    "message" : [
      "Foreach writer has been aborted due to a task failure"
    ]
  },
  "_LEGACY_ERROR_TEMP_2258" : {
    "message" : [
      "Error reading delta file <fileToRead> of <clazz>: key size cannot be <keySize>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2259" : {
    "message" : [
      "Error reading snapshot file <fileToRead> of <clazz>: <message>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2260" : {
    "message" : [
      "Cannot purge as it might break internal state."
    ]
  },
  "_LEGACY_ERROR_TEMP_2261" : {
    "message" : [
      "Clean up source files is not supported when reading from the output directory of FileStreamSink."
    ]
  },
  "_LEGACY_ERROR_TEMP_2262" : {
    "message" : [
      "latestOffset(Offset, ReadLimit) should be called instead of this method"
    ]
  },
  "_LEGACY_ERROR_TEMP_2263" : {
    "message" : [
      "Error: we detected a possible problem with the location of your checkpoint and you",
      "likely need to move it before restarting this query.",
      "",
      "Earlier version of Spark incorrectly escaped paths when writing out checkpoints for",
      "structured streaming. While this was corrected in Spark 3.0, it appears that your",
      "query was started using an earlier version that incorrectly handled the checkpoint",
      "path.",
      "",
      "Correct Checkpoint Directory: <checkpointPath>",
      "Incorrect Checkpoint Directory: <legacyCheckpointDir>",
      "",
      "Please move the data from the incorrect directory to the correct one, delete the",
      "incorrect directory, and then restart this query. If you believe you are receiving",
      "this message in error, you can disable it with the SQL conf",
      "<StreamingCheckpointEscapedPathCheckEnabled>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2264" : {
    "message" : [
      "Subprocess exited with status <exitCode>. Error: <stderrBuffer>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2265" : {
    "message" : [
      "<nodeName> without serde does not support <dt> as output data type"
    ]
  },
  "_LEGACY_ERROR_TEMP_2266" : {
    "message" : [
      "Invalid `startIndex` provided for generating iterator over the array. Total elements: <numRows>, requested `startIndex`: <startIndex>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2267" : {
    "message" : [
      "The backing <className> has been modified since the creation of this Iterator"
    ]
  },
  "_LEGACY_ERROR_TEMP_2268" : {
    "message" : [
      "<nodeName> does not implement doExecuteBroadcast"
    ]
  },
  "_LEGACY_ERROR_TEMP_2269" : {
    "message" : [
      "<globalTempDB> is a system preserved database, please rename your existing database to resolve the name conflict, or set a different value for <globalTempDatabase>, and launch your Spark application again."
    ]
  },
  "_LEGACY_ERROR_TEMP_2270" : {
    "message" : [
      "comment on table is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2271" : {
    "message" : [
      "UpdateColumnNullability is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2272" : {
    "message" : [
      "Rename column is only supported for MySQL version 8.0 and above."
    ]
  },
  "_LEGACY_ERROR_TEMP_2273" : {
    "message" : [
      "<message>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2274" : {
    "message" : [
      "Nested field <colName> is not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_2275" : {
    "message" : [
      "Dataset transformations and actions can only be invoked by the driver, not inside of other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702."
    ]
  },
  "_LEGACY_ERROR_TEMP_2276" : {
    "message" : [
      "Hive table <tableName> with ANSI intervals is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2277" : {
    "message" : [
      "Number of dynamic partitions created is <numWrittenParts>, which is more than <maxDynamicPartitions>. To solve this try to set <maxDynamicPartitionsKey> to at least <numWrittenParts>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2278" : {
    "message" : [
      "The input <valueType> '<input>' does not match the given number format: '<format>'"
    ]
  },
  "_LEGACY_ERROR_TEMP_2279" : {
    "message" : [
      "Multiple bucket transforms are not supported."
    ]
  },
  "_LEGACY_ERROR_TEMP_2280" : {
    "message" : [
      "Create namespace comment is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2281" : {
    "message" : [
      "Remove namespace comment is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2282" : {
    "message" : [
      "Drop namespace restrict is not supported"
    ]
  },
  "_LEGACY_ERROR_TEMP_2300" : {
    "message" : [
      "The number of lambda function arguments '<namesSize>' does not match the number of arguments expected by the higher order function '<argInfoSize>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_2301" : {
    "message" : [
      "Lambda function arguments should not have names that are semantically the same."
    ]
  },
  "_LEGACY_ERROR_TEMP_2302" : {
    "message" : [
      "'<name>' does not support more than one sources"
    ]
  },
  "_LEGACY_ERROR_TEMP_2303" : {
    "message" : [
      "incompatible types found in column <name> for inline table"
    ]
  },
  "_LEGACY_ERROR_TEMP_2304" : {
    "message" : [
      "cannot evaluate expression <sqlExpr> in inline table definition"
    ]
  },
  "_LEGACY_ERROR_TEMP_2305" : {
    "message" : [
      "expected <numCols> columns but found <rowSize> columns in row <ri>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2306" : {
    "message" : [
      "A lambda function should only be used in a higher order function. However, its class is <class>, which is not a higher order function."
    ]
  },
  "_LEGACY_ERROR_TEMP_2307" : {
    "message" : [
      "Number of given aliases does not match number of output columns. Function name: <funcName>; number of aliases: <aliasesNum>; number of output columns: <outColsNum>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2308" : {
    "message" : [
      "could not resolve `<name>` to a table-valued function"
    ]
  },
  "_LEGACY_ERROR_TEMP_2309" : {
    "message" : [
      "cannot resolve <sqlExpr> in MERGE command given columns [<cols>]"
    ]
  },
  "_LEGACY_ERROR_TEMP_2310" : {
    "message" : [
      "'writeStream' can be called only on streaming Dataset/DataFrame"
    ]
  },
  "_LEGACY_ERROR_TEMP_2311" : {
    "message" : [
      "'writeTo' can not be called on streaming Dataset/DataFrame"
    ]
  },
  "_LEGACY_ERROR_TEMP_2312" : {
    "message" : [
      "'write' can not be called on streaming Dataset/DataFrame"
    ]
  },
  "_LEGACY_ERROR_TEMP_2313" : {
    "message" : [
      "Hint not found: <name>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2314" : {
    "message" : [
      "cannot resolve '<sqlExpr>' due to argument data type mismatch: <msg>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2315" : {
    "message" : [
      "cannot resolve '<sqlExpr>' due to data type mismatch: <msg><hint>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2316" : {
    "message" : [
      "observed metrics should be named: <operator>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2317" : {
    "message" : [
      "window expressions are not allowed in observed metrics, but found: <sqlExpr>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2318" : {
    "message" : [
      "non-deterministic expression <sqlExpr> can only be used as an argument to an aggregate function."
    ]
  },
  "_LEGACY_ERROR_TEMP_2319" : {
    "message" : [
      "nested aggregates are not allowed in observed metrics, but found: <sqlExpr>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2320" : {
    "message" : [
      "distinct aggregates are not allowed in observed metrics, but found: <sqlExpr>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2321" : {
    "message" : [
      "aggregates with filter predicate are not allowed in observed metrics, but found: <sqlExpr>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2322" : {
    "message" : [
      "attribute <sqlExpr> can only be used as an argument to an aggregate function."
    ]
  },
  "_LEGACY_ERROR_TEMP_2323" : {
    "message" : [
      "Cannot <op> column, because <fieldNames> already exists in <struct>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2324" : {
    "message" : [
      "Cannot update <table> field <fieldName> type: update a struct by updating its fields"
    ]
  },
  "_LEGACY_ERROR_TEMP_2325" : {
    "message" : [
      "Cannot update <table> field <fieldName> type: update a map by updating <fieldName>.key or <fieldName>.value"
    ]
  },
  "_LEGACY_ERROR_TEMP_2326" : {
    "message" : [
      "Cannot update <table> field <fieldName> type: update the element by updating <fieldName>.element"
    ]
  },
  "_LEGACY_ERROR_TEMP_2327" : {
    "message" : [
      "Cannot update <table> field <fieldName> type: update a UserDefinedType[<udtSql>] by updating its fields"
    ]
  },
  "_LEGACY_ERROR_TEMP_2328" : {
    "message" : [
      "Cannot update <table> field <fieldName> to interval type"
    ]
  },
  "_LEGACY_ERROR_TEMP_2329" : {
    "message" : [
      "Cannot update <table> field <fieldName>: <oldType> cannot be cast to <newType>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2330" : {
    "message" : [
      "Cannot change nullable column to non-nullable: <fieldName>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2331" : {
    "message" : [
      "failed to evaluate expression <sqlExpr>: <msg>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2332" : {
    "message" : [
      "<msg>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2400" : {
    "message" : [
      "The <name> expression must evaluate to a constant value, but got <limitExpr>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2401" : {
    "message" : [
      "The <name> expression must be integer type, but got <dataType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2402" : {
    "message" : [
      "The evaluated <name> expression must not be null, but got <limitExpr>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2403" : {
    "message" : [
      "The <name> expression must be equal to or greater than 0, but got <v>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2404" : {
    "message" : [
      "Table <name> is not partitioned."
    ]
  },
  "_LEGACY_ERROR_TEMP_2405" : {
    "message" : [
      "Table <name> does not support partition management."
    ]
  },
  "_LEGACY_ERROR_TEMP_2406" : {
    "message" : [
      "invalid cast from <srcType> to <targetType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2407" : {
    "message" : [
      "grouping_id() can only be used with GroupingSets/Cube/Rollup"
    ]
  },
  "_LEGACY_ERROR_TEMP_2408" : {
    "message" : [
      "Window function <w> requires an OVER clause."
    ]
  },
  "_LEGACY_ERROR_TEMP_2409" : {
    "message" : [
      "Distinct window functions are not supported: <w>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2410" : {
    "message" : [
      "<wf> function can only be evaluated in an ordered row-based window frame with a single offset: <w>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2411" : {
    "message" : [
      "Cannot specify order by or frame for '<aggFunc>'."
    ]
  },
  "_LEGACY_ERROR_TEMP_2412" : {
    "message" : [
      "Expression '<sqlExpr>' not supported within a window function."
    ]
  },
  "_LEGACY_ERROR_TEMP_2413" : {
    "message" : [
      "Input argument to <argName> must be a constant."
    ]
  },
  "_LEGACY_ERROR_TEMP_2414" : {
    "message" : [
      "Event time must be defined on a window or a timestamp, but <evName> is of type <evType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2415" : {
    "message" : [
      "filter expression '<filter>' of type <type> is not a boolean."
    ]
  },
  "_LEGACY_ERROR_TEMP_2416" : {
    "message" : [
      "join condition '<join>' of type <type> is not a boolean."
    ]
  },
  "_LEGACY_ERROR_TEMP_2417" : {
    "message" : [
      "join condition '<condition>' of type <dataType> is not a boolean."
    ]
  },
  "_LEGACY_ERROR_TEMP_2418" : {
    "message" : [
      "Input argument tolerance must be a constant."
    ]
  },
  "_LEGACY_ERROR_TEMP_2419" : {
    "message" : [
      "Input argument tolerance must be non-negative."
    ]
  },
  "_LEGACY_ERROR_TEMP_2420" : {
    "message" : [
      "It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query."
    ]
  },
  "_LEGACY_ERROR_TEMP_2421" : {
    "message" : [
      "nondeterministic expression <sqlExpr> should not appear in the arguments of an aggregate function."
    ]
  },
  "_LEGACY_ERROR_TEMP_2422" : {
    "message" : [
      "grouping expressions sequence is empty, and '<sqlExpr>' is not an aggregate function. Wrap '<aggExprs>' in windowing function(s) or wrap '<sqlExpr>' in first() (or first_value) if you don't care which value you get."
    ]
  },
  "_LEGACY_ERROR_TEMP_2423" : {
    "message" : [
      "Correlated scalar subquery '<sqlExpr>' is neither present in the group by, nor in an aggregate function. Add it to group by using ordinal position or wrap it in first() (or first_value) if you don't care which value you get."
    ]
  },
  "_LEGACY_ERROR_TEMP_2424" : {
    "message" : [
      "aggregate functions are not allowed in GROUP BY, but found <sqlExpr>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2425" : {
    "message" : [
      "expression <sqlExpr> cannot be used as a grouping expression because its data type <dataType> is not an orderable data type."
    ]
  },
  "_LEGACY_ERROR_TEMP_2426" : {
    "message" : [
      "nondeterministic expression <sqlExpr> should not appear in grouping expression."
    ]
  },
  "_LEGACY_ERROR_TEMP_2427" : {
    "message" : [
      "sorting is not supported for columns of type <type>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2428" : {
    "message" : [
      "The sum of the LIMIT clause and the OFFSET clause must not be greater than the maximum 32-bit integer value (2,147,483,647) but found limit = <limit>, offset = <offset>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2430" : {
    "message" : [
      "<operator> can only be performed on tables with compatible column types. The <ci> column of the <ti> table is <dt1> type which is not compatible with <dt2> at the same column of the first table.<hint>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2431" : {
    "message" : [
      "Invalid partitioning: <cols> is missing or is in a map or array"
    ]
  },
  "_LEGACY_ERROR_TEMP_2432" : {
    "message" : [
      "<msg>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2433" : {
    "message" : [
      "Only a single table generating function is allowed in a SELECT clause, found:",
      "<sqlExprs>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2434" : {
    "message" : [
      "Failure when resolving conflicting references in Join:",
      "<plan>",
      "Conflicting attributes: <conflictingAttributes>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2435" : {
    "message" : [
      "Failure when resolving conflicting references in Intersect:",
      "<plan>",
      "Conflicting attributes: <conflictingAttributes>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2436" : {
    "message" : [
      "Failure when resolving conflicting references in Except:",
      "<plan>",
      "Conflicting attributes: <conflictingAttributes>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2437" : {
    "message" : [
      "Failure when resolving conflicting references in AsOfJoin:",
      "<plan>",
      "Conflicting attributes: <conflictingAttributes>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2438" : {
    "message" : [
      "Cannot have map type columns in DataFrame which calls set operations(intersect, except, etc.), but the type of column <colName> is <dataType>."
    ]
  },
  "_LEGACY_ERROR_TEMP_2439" : {
    "message" : [
      "nondeterministic expressions are only allowed in Project, Filter, Aggregate or Window, found:",
      "<sqlExprs>",
      "in operator <operator>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2440" : {
    "message" : [
      "Aggregate/Window/Generate expressions are not valid in where clause of the query.",
      "Expression in where clause: [<condition>]",
      "Invalid expressions: [<invalidExprSqls>]"
    ]
  },
  "_LEGACY_ERROR_TEMP_2441" : {
    "message" : [
      "The query operator `<operator>` contains one or more unsupported expression types Aggregate, Window or Generate.",
      "Invalid expressions: [<invalidExprSqls>]"
    ]
  },
  "_LEGACY_ERROR_TEMP_2442" : {
    "message" : [
      "unresolved operator <operator>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2443" : {
    "message" : [
      "Multiple definitions of observed metrics named '<name>': <plan>"
    ]
  },
  "_LEGACY_ERROR_TEMP_2444" : {
    "message" : [
      "Function '<funcName>' does not implement ScalarFunction or AggregateFunction"
    ]
  },
  "_LEGACY_ERROR_TEMP_2445" : {
    "message" : [
      "grouping() can only be used with GroupingSets/Cube/Rollup"
    ]
  }
}
