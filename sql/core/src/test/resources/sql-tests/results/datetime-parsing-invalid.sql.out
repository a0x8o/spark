-- Automatically generated by SQLQueryTestSuite
-- !query
select to_timestamp('294248', 'y')
-- !query schema
struct<>
-- !query output
java.lang.ArithmeticException
long overflow


-- !query
select to_timestamp('1', 'yy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.


-- !query
select to_timestamp('-12', 'yy')
-- !query schema
struct<to_timestamp(-12, yy):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('123', 'yy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '123' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.


-- !query
select to_timestamp('1', 'yyy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.


-- !query
select to_timestamp('1234567', 'yyyyyyy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to recognize 'yyyyyyy' pattern in the DateTimeFormatter. 1) You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select to_timestamp('366', 'D')
-- !query schema
struct<to_timestamp(366, D):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('9', 'DD')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '9' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.


-- !query
select to_timestamp('9', 'DDD')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '9' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.


-- !query
select to_timestamp('99', 'DDD')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '99' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.


-- !query
select to_timestamp('30-365', 'dd-DDD')
-- !query schema
struct<to_timestamp(30-365, dd-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('11-365', 'MM-DDD')
-- !query schema
struct<to_timestamp(11-365, MM-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('2019-366', 'yyyy-DDD')
-- !query schema
struct<to_timestamp(2019-366, yyyy-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('12-30-365', 'MM-dd-DDD')
-- !query schema
struct<to_timestamp(12-30-365, MM-dd-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('2020-01-365', 'yyyy-dd-DDD')
-- !query schema
struct<to_timestamp(2020-01-365, yyyy-dd-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('2020-10-350', 'yyyy-MM-DDD')
-- !query schema
struct<to_timestamp(2020-10-350, yyyy-MM-DDD):timestamp>
-- !query output
NULL


-- !query
select to_timestamp('2020-11-31-366', 'yyyy-MM-dd-DDD')
-- !query schema
struct<to_timestamp(2020-11-31-366, yyyy-MM-dd-DDD):timestamp>
-- !query output
NULL


-- !query
select from_csv('2018-366', 'date Date', map('dateFormat', 'yyyy-DDD'))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2018-366' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.


-- !query
select to_date("2020-01-27T20:06:11.847", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<to_date(2020-01-27T20:06:11.847, yyyy-MM-dd HH:mm:ss.SSS):date>
-- !query output
NULL


-- !query
select to_date("Unparseable", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<to_date(Unparseable, yyyy-MM-dd HH:mm:ss.SSS):date>
-- !query output
NULL


-- !query
select to_timestamp("2020-01-27T20:06:11.847", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<to_timestamp(2020-01-27T20:06:11.847, yyyy-MM-dd HH:mm:ss.SSS):timestamp>
-- !query output
NULL


-- !query
select to_timestamp("Unparseable", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<to_timestamp(Unparseable, yyyy-MM-dd HH:mm:ss.SSS):timestamp>
-- !query output
NULL


-- !query
select unix_timestamp("2020-01-27T20:06:11.847", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<unix_timestamp(2020-01-27T20:06:11.847, yyyy-MM-dd HH:mm:ss.SSS):bigint>
-- !query output
NULL


-- !query
select unix_timestamp("Unparseable", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<unix_timestamp(Unparseable, yyyy-MM-dd HH:mm:ss.SSS):bigint>
-- !query output
NULL


-- !query
select to_unix_timestamp("2020-01-27T20:06:11.847", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<to_unix_timestamp(2020-01-27T20:06:11.847, yyyy-MM-dd HH:mm:ss.SSS):bigint>
-- !query output
NULL


-- !query
select to_unix_timestamp("Unparseable", "yyyy-MM-dd HH:mm:ss.SSS")
-- !query schema
struct<to_unix_timestamp(Unparseable, yyyy-MM-dd HH:mm:ss.SSS):bigint>
-- !query output
NULL


-- !query
select cast("Unparseable" as timestamp)
-- !query schema
struct<CAST(Unparseable AS TIMESTAMP):timestamp>
-- !query output
NULL


-- !query
select cast("Unparseable" as date)
-- !query schema
struct<CAST(Unparseable AS DATE):date>
-- !query output
NULL
