-- Automatically generated by SQLQueryTestSuite
-- !query
drop table if exists t
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t


-- !query
create table t(x int, y string) using csv
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false


-- !query
insert into t values (0, 'abc'), (1, 'def')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [x, y]
+- Project [cast(col1#x as int) AS x#x, cast(col2#x as string) AS y#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
drop table if exists other
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.other


-- !query
create table other(a int, b int) using json
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`other`, false


-- !query
insert into other values (1, 1), (1, 2), (2, 4)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/other, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/other], Append, `spark_catalog`.`default`.`other`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/other), [a, b]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
drop table if exists st
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.st


-- !query
create table st(x int, col struct<i1:int, i2:int>) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`st`, false


-- !query
insert into st values (1, (2, 3))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/st, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/st], Append, `spark_catalog`.`default`.`st`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/st), [x, col]
+- Project [cast(col1#x as int) AS x#x, named_struct(i1, cast(col2#x.col1 as int), i2, cast(col2#x.col2 as int)) AS col#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
table t
|> select 1 as x
-- !query analysis
Project [pipeselect(1) AS x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x, y
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x, y
|> select x + length(y) as z
-- !query analysis
Project [pipeselect((x#x + length(y#x))) AS z#x]
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
values (0), (1) tab(col)
|> select col * 2 as result
-- !query analysis
Project [pipeselect((col#x * 2)) AS result#x]
+- SubqueryAlias tab
   +- LocalRelation [col#x]


-- !query
(select * from t union all select * from t)
|> select x + length(y) as result
-- !query analysis
Project [pipeselect((x#x + length(y#x))) AS result#x]
+- Union false, false
   :- Project [x#x, y#x]
   :  +- SubqueryAlias spark_catalog.default.t
   :     +- Relation spark_catalog.default.t[x#x,y#x] csv
   +- Project [x#x, y#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(table t
 |> select x, y
 |> select x)
union all
select x from t where x < 1
-- !query analysis
Union false, false
:- Project [x#x]
:  +- Project [x#x, y#x]
:     +- SubqueryAlias spark_catalog.default.t
:        +- Relation spark_catalog.default.t[x#x,y#x] csv
+- Project [x#x]
   +- Filter (x#x < 1)
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select col from st)
|> select col.i1
-- !query analysis
Project [col#x.i1 AS i1#x]
+- Project [col#x]
   +- SubqueryAlias spark_catalog.default.st
      +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table st
|> select st.col.i1
-- !query analysis
Project [col#x.i1 AS i1#x]
+- SubqueryAlias spark_catalog.default.st
   +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table t
|> select (select a from other where x = a limit 1) as result
-- !query analysis
Project [pipeselect(scalar-subquery#x [x#x]) AS result#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Project [a#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
select (values (0) tab(col) |> select col) as result
-- !query analysis
Project [scalar-subquery#x [] AS result#x]
:  +- Project [col#x]
:     +- SubqueryAlias tab
:        +- LocalRelation [col#x]
+- OneRowRelation


-- !query
table t
|> select (select any_value(a) from other where x = a limit 1) as result
-- !query analysis
Project [pipeselect(scalar-subquery#x [x#x]) AS result#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Aggregate [any_value(a#x, false) AS any_value(a)#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x + length(x) as z, z + 1 as plus_one
-- !query analysis
Project [z#x, pipeselect((z#x + 1)) AS plus_one#x]
+- Project [x#x, y#x, pipeselect((x#x + length(cast(x#x as string)))) AS z#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select first_value(x) over (partition by y) as result
-- !query analysis
Project [result#x]
+- Project [x#x, y#x, _we0#x, pipeselect(_we0#x) AS result#x]
   +- Window [first_value(x#x, false) windowspecdefinition(y#x, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#x], [y#x]
      +- Project [x#x, y#x]
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
select 1 x, 2 y, 3 z
|> select 1 + sum(x) over (),
     avg(y) over (),
     x,
     avg(x+1) over (partition by y order by z) AS a2
|> select a2
-- !query analysis
Project [a2#x]
+- Project [(1 + sum(x) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING))#xL, avg(y) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x, x#x, a2#x]
   +- Project [x#x, y#x, _w1#x, z#x, _we0#xL, avg(y) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x, _we2#x, (cast(1 as bigint) + _we0#xL) AS (1 + sum(x) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING))#xL, avg(y) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x, pipeselect(_we2#x) AS a2#x]
      +- Window [avg(_w1#x) windowspecdefinition(y#x, z#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we2#x], [y#x], [z#x ASC NULLS FIRST]
         +- Window [sum(x#x) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#xL, avg(y#x) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS avg(y) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x]
            +- Project [x#x, y#x, (x#x + 1) AS _w1#x, z#x]
               +- Project [1 AS x#x, 2 AS y#x, 3 AS z#x]
                  +- OneRowRelation


-- !query
table t
|> select x, count(*) over ()
|> select x
-- !query analysis
Project [x#x]
+- Project [x#x, count(1) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL]
   +- Project [x#x, count(1) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL, count(1) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL]
      +- Window [count(1) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS count(1) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL]
         +- Project [x#x]
            +- SubqueryAlias spark_catalog.default.t
               +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select distinct x, y
-- !query analysis
Distinct
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select *
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select * except (y)
-- !query analysis
Project [x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select /*+ repartition(3) */ *
-- !query analysis
Repartition 3, true
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select /*+ repartition(3) */ distinct x
-- !query analysis
Repartition 3, true
+- Distinct
   +- Project [x#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select /*+ repartition(3) */ all x
-- !query analysis
Repartition 3, true
+- Project [x#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select sum(x) as result
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_SELECT_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 24,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> select y, length(y) + sum(x) as result
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_SELECT_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 34,
    "stopIndex" : 39,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> where true
-- !query analysis
Filter true
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where x + length(y) < 4
-- !query analysis
Filter ((x#x + length(y#x)) < 4)
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where x + length(y) < 4
|> where x + length(y) < 3
-- !query analysis
Filter ((x#x + length(y#x)) < 3)
+- SubqueryAlias __auto_generated_subquery_name
   +- Filter ((x#x + length(y#x)) < 4)
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select x, sum(length(y)) as sum_len from t group by x)
|> where x = 1
-- !query analysis
Filter (x#x = 1)
+- SubqueryAlias __auto_generated_subquery_name
   +- Aggregate [x#x], [x#x, sum(length(y#x)) AS sum_len#xL]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where t.x = 1
-- !query analysis
Filter (x#x = 1)
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where spark_catalog.default.t.x = 1
-- !query analysis
Filter (x#x = 1)
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select col from st)
|> where col.i1 = 1
-- !query analysis
Filter (col#x.i1 = 1)
+- SubqueryAlias __auto_generated_subquery_name
   +- Project [col#x]
      +- SubqueryAlias spark_catalog.default.st
         +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table st
|> where st.col.i1 = 2
-- !query analysis
Filter (col#x.i1 = 2)
+- SubqueryAlias spark_catalog.default.st
   +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table t
|> where exists (select a from other where x = a limit 1)
-- !query analysis
Filter exists#x [x#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Project [a#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where (select any_value(a) from other where x = a limit 1) = 1
-- !query analysis
Filter (scalar-subquery#x [x#x] = 1)
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Aggregate [any_value(a#x, false) AS any_value(a)#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where sum(x) = 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_WHERE_CONDITION",
  "sqlState" : "42903",
  "messageParameters" : {
    "condition" : "\"(sum(x) = 1)\"",
    "expressionList" : "sum(spark_catalog.default.t.x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 27,
    "fragment" : "table t\n|> where sum(x) = 1"
  } ]
}


-- !query
table t
|> where y = 'abc' or length(y) + sum(x) = 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_WHERE_CONDITION",
  "sqlState" : "42903",
  "messageParameters" : {
    "condition" : "\"((y = abc) OR ((length(y) + sum(x)) = 1))\"",
    "expressionList" : "sum(spark_catalog.default.t.x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 52,
    "fragment" : "table t\n|> where y = 'abc' or length(y) + sum(x) = 1"
  } ]
}


-- !query
table t
|> where first_value(x) over (partition by y) = 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_1034",
  "messageParameters" : {
    "clauseName" : "WHERE"
  }
}


-- !query
select * from t where first_value(x) over (partition by y) = 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_1034",
  "messageParameters" : {
    "clauseName" : "WHERE"
  }
}


-- !query
table t
|> select x, length(y) as z
|> where x + length(y) < 4
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`x`, `z`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 57,
    "stopIndex" : 57,
    "fragment" : "y"
  } ]
}


-- !query
(select x, sum(length(y)) as sum_len from t group by x)
|> where sum(length(y)) = 3
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`x`, `sum_len`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 77,
    "stopIndex" : 77,
    "fragment" : "y"
  } ]
}


-- !query
drop table t
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t


-- !query
drop table other
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.other


-- !query
drop table st
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.st
