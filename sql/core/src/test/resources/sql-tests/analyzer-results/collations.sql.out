-- Automatically generated by SQLQueryTestSuite
-- !query
create table t1(ucs_basic string collate ucs_basic, ucs_basic_lcase string collate ucs_basic_lcase) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t1`, false


-- !query
insert into t1 values('aaa', 'aaa')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [ucs_basic, ucs_basic_lcase]
+- Project [cast(col1#x as string) AS ucs_basic#x, cast(col2#x as string COLLATE UCS_BASIC_LCASE) AS ucs_basic_lcase#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
insert into t1 values('AAA', 'AAA')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [ucs_basic, ucs_basic_lcase]
+- Project [cast(col1#x as string) AS ucs_basic#x, cast(col2#x as string COLLATE UCS_BASIC_LCASE) AS ucs_basic_lcase#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
insert into t1 values('bbb', 'bbb')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [ucs_basic, ucs_basic_lcase]
+- Project [cast(col1#x as string) AS ucs_basic#x, cast(col2#x as string COLLATE UCS_BASIC_LCASE) AS ucs_basic_lcase#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
insert into t1 values('BBB', 'BBB')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [ucs_basic, ucs_basic_lcase]
+- Project [cast(col1#x as string) AS ucs_basic#x, cast(col2#x as string COLLATE UCS_BASIC_LCASE) AS ucs_basic_lcase#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
select count(*) from t1 group by ucs_basic
-- !query analysis
Aggregate [ucs_basic#x], [count(1) AS count(1)#xL]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet


-- !query
select count(*) from t1 group by ucs_basic_lcase
-- !query analysis
Aggregate [ucs_basic_lcase#x], [count(1) AS count(1)#xL]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet


-- !query
select * from t1 where ucs_basic = 'aaa'
-- !query analysis
Project [ucs_basic#x, ucs_basic_lcase#x]
+- Filter (ucs_basic#x = aaa)
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet


-- !query
select * from t1 where ucs_basic_lcase = 'aaa' collate ucs_basic_lcase
-- !query analysis
Project [ucs_basic#x, ucs_basic_lcase#x]
+- Filter (ucs_basic_lcase#x = collate(aaa, ucs_basic_lcase))
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet


-- !query
select * from t1 where ucs_basic < 'bbb'
-- !query analysis
Project [ucs_basic#x, ucs_basic_lcase#x]
+- Filter (ucs_basic#x < bbb)
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet


-- !query
select * from t1 where ucs_basic_lcase < 'bbb' collate ucs_basic_lcase
-- !query analysis
Project [ucs_basic#x, ucs_basic_lcase#x]
+- Filter (ucs_basic_lcase#x < collate(bbb, ucs_basic_lcase))
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet


-- !query
select l.ucs_basic, r.ucs_basic_lcase from t1 l join t1 r on l.ucs_basic_lcase = r.ucs_basic_lcase
-- !query analysis
Project [ucs_basic#x, ucs_basic_lcase#x]
+- Join Inner, (ucs_basic_lcase#x = ucs_basic_lcase#x)
   :- SubqueryAlias l
   :  +- SubqueryAlias spark_catalog.default.t1
   :     +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet
   +- SubqueryAlias r
      +- SubqueryAlias spark_catalog.default.t1
         +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet


-- !query
create table t2(ucs_basic string collate ucs_basic, ucs_basic_lcase string collate ucs_basic_lcase) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t2`, false


-- !query
insert into t2 values('aaa', 'aaa')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [ucs_basic, ucs_basic_lcase]
+- Project [cast(col1#x as string) AS ucs_basic#x, cast(col2#x as string COLLATE UCS_BASIC_LCASE) AS ucs_basic_lcase#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
insert into t2 values('bbb', 'bbb')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [ucs_basic, ucs_basic_lcase]
+- Project [cast(col1#x as string) AS ucs_basic#x, cast(col2#x as string COLLATE UCS_BASIC_LCASE) AS ucs_basic_lcase#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
select * from t1 anti join t2 on t1.ucs_basic_lcase = t2.ucs_basic_lcase
-- !query analysis
Project [ucs_basic#x, ucs_basic_lcase#x]
+- Join LeftAnti, (ucs_basic_lcase#x = ucs_basic_lcase#x)
   :- SubqueryAlias spark_catalog.default.t1
   :  +- Relation spark_catalog.default.t1[ucs_basic#x,ucs_basic_lcase#x] parquet
   +- SubqueryAlias spark_catalog.default.t2
      +- Relation spark_catalog.default.t2[ucs_basic#x,ucs_basic_lcase#x] parquet


-- !query
drop table t2
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t2


-- !query
drop table t1
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t1


-- !query
create table t1 (c1 struct<ucs_basic: string collate ucs_basic, ucs_basic_lcase: string collate ucs_basic_lcase>) USING PARQUET
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t1`, false


-- !query
INSERT INTO t1 VALUES (named_struct('ucs_basic', 'aaa', 'ucs_basic_lcase', 'aaa'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [c1]
+- Project [named_struct(ucs_basic, col1#x.ucs_basic, ucs_basic_lcase, cast(col1#x.ucs_basic_lcase as string COLLATE UCS_BASIC_LCASE)) AS c1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO t1 VALUES (named_struct('ucs_basic', 'AAA', 'ucs_basic_lcase', 'AAA'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [c1]
+- Project [named_struct(ucs_basic, col1#x.ucs_basic, ucs_basic_lcase, cast(col1#x.ucs_basic_lcase as string COLLATE UCS_BASIC_LCASE)) AS c1#x]
   +- LocalRelation [col1#x]


-- !query
select count(*) from t1 group by c1.ucs_basic
-- !query analysis
Aggregate [c1#x.ucs_basic], [count(1) AS count(1)#xL]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[c1#x] parquet


-- !query
select count(*) from t1 group by c1.ucs_basic_lcase
-- !query analysis
Aggregate [c1#x.ucs_basic_lcase], [count(1) AS count(1)#xL]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[c1#x] parquet


-- !query
drop table t1
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t1
