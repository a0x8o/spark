/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.sql.internal

import org.apache.spark.sql.api.java._

/**
 * Helper class that provides conversions from org.apache.spark.sql.api.java.Function* to
 * scala.Function*.
 */
private[sql] object ToScalaUDF {
  // scalastyle:off line.size.limit

  /* register 0-22 were generated by this script

    (0 to 22).foreach { i =>
      val extTypeArgs = (0 to i).map(_ => "_").mkString(", ")
      val anyTypeArgs = (0 to i).map(_ => "Any").mkString(", ")
      val anyCast = s".asInstanceOf[UDF$i[$anyTypeArgs]]"
      val anyParams = (1 to i).map(_ => "_: Any").mkString(", ")
      val funcCall = if (i == 0) s"() => f$anyCast.call($anyParams)" else s"f$anyCast.call($anyParams)"
      println(s"""
        |/**
        | * Create a scala.Function$i wrapper for a org.apache.spark.sql.api.java.UDF$i instance.
        | */
        |def apply(f: UDF$i[$extTypeArgs]): AnyRef = {
        |  $funcCall
        |}""".stripMargin)
    }
    */

  /**
   * Create a scala.Function0 wrapper for a org.apache.spark.sql.api.java.UDF0 instance.
   */
  def apply(f: UDF0[_]): AnyRef = {
    () => f.asInstanceOf[UDF0[Any]].call()
  }

  /**
   * Create a scala.Function1 wrapper for a org.apache.spark.sql.api.java.UDF1 instance.
   */
  def apply(f: UDF1[_, _]): AnyRef = {
    f.asInstanceOf[UDF1[Any, Any]].call(_: Any)
  }

  /**
   * Create a scala.Function2 wrapper for a org.apache.spark.sql.api.java.UDF2 instance.
   */
  def apply(f: UDF2[_, _, _]): AnyRef = {
    f.asInstanceOf[UDF2[Any, Any, Any]].call(_: Any, _: Any)
  }

  /**
   * Create a scala.Function3 wrapper for a org.apache.spark.sql.api.java.UDF3 instance.
   */
  def apply(f: UDF3[_, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF3[Any, Any, Any, Any]].call(_: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function4 wrapper for a org.apache.spark.sql.api.java.UDF4 instance.
   */
  def apply(f: UDF4[_, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF4[Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function5 wrapper for a org.apache.spark.sql.api.java.UDF5 instance.
   */
  def apply(f: UDF5[_, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF5[Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function6 wrapper for a org.apache.spark.sql.api.java.UDF6 instance.
   */
  def apply(f: UDF6[_, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF6[Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function7 wrapper for a org.apache.spark.sql.api.java.UDF7 instance.
   */
  def apply(f: UDF7[_, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF7[Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function8 wrapper for a org.apache.spark.sql.api.java.UDF8 instance.
   */
  def apply(f: UDF8[_, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF8[Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function9 wrapper for a org.apache.spark.sql.api.java.UDF9 instance.
   */
  def apply(f: UDF9[_, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF9[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function10 wrapper for a org.apache.spark.sql.api.java.UDF10 instance.
   */
  def apply(f: UDF10[_, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF10[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function11 wrapper for a org.apache.spark.sql.api.java.UDF11 instance.
   */
  def apply(f: UDF11[_, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF11[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function12 wrapper for a org.apache.spark.sql.api.java.UDF12 instance.
   */
  def apply(f: UDF12[_, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF12[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function13 wrapper for a org.apache.spark.sql.api.java.UDF13 instance.
   */
  def apply(f: UDF13[_, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF13[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function14 wrapper for a org.apache.spark.sql.api.java.UDF14 instance.
   */
  def apply(f: UDF14[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF14[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function15 wrapper for a org.apache.spark.sql.api.java.UDF15 instance.
   */
  def apply(f: UDF15[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF15[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function16 wrapper for a org.apache.spark.sql.api.java.UDF16 instance.
   */
  def apply(f: UDF16[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF16[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function17 wrapper for a org.apache.spark.sql.api.java.UDF17 instance.
   */
  def apply(f: UDF17[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF17[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function18 wrapper for a org.apache.spark.sql.api.java.UDF18 instance.
   */
  def apply(f: UDF18[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF18[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function19 wrapper for a org.apache.spark.sql.api.java.UDF19 instance.
   */
  def apply(f: UDF19[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF19[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function20 wrapper for a org.apache.spark.sql.api.java.UDF20 instance.
   */
  def apply(f: UDF20[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF20[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function21 wrapper for a org.apache.spark.sql.api.java.UDF21 instance.
   */
  def apply(f: UDF21[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF21[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }

  /**
   * Create a scala.Function22 wrapper for a org.apache.spark.sql.api.java.UDF22 instance.
   */
  def apply(f: UDF22[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]): AnyRef = {
    f.asInstanceOf[UDF22[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
  }
  // scalastyle:on line.size.limit
}
