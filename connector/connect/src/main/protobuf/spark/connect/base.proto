/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = 'proto3';

package spark.connect;

import "google/protobuf/any.proto";
import "spark/connect/commands.proto";
import "spark/connect/relations.proto";
import "spark/connect/types.proto";

option java_multiple_files = true;
option java_package = "org.apache.spark.connect.proto";

// A [[Plan]] is the structure that carries the runtime information for the execution from the
// client to the server. A [[Plan]] can either be of the type [[Relation]] which is a reference
// to the underlying logical plan or it can be of the [[Command]] type that is used to execute
// commands on the server.
message Plan {
  oneof op_type {
    Relation root = 1;
    Command command = 2;
  }
}

// Explains the input plan based on a configurable mode.
message Explain {
  // Plan explanation mode.
  enum ExplainMode {
    MODE_UNSPECIFIED = 0;

    // Generates only physical plan.
    SIMPLE = 1;

    // Generates parsed logical plan, analyzed logical plan, optimized logical plan and physical plan.
    // Parsed Logical plan is a unresolved plan that extracted from the query. Analyzed logical plans
    // transforms which translates unresolvedAttribute and unresolvedRelation into fully typed objects.
    // The optimized logical plan transforms through a set of optimization rules, resulting in the
    // physical plan.
    EXTENDED = 2;

    // Generates code for the statement, if any and a physical plan.
    CODEGEN = 3;

    // If plan node statistics are available, generates a logical plan and also the statistics.
    COST = 4;

    // Generates a physical plan outline and also node details.
    FORMATTED = 5;
  }

  // (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
  ExplainMode explain_mode= 1;
}

// User Context is used to refer to one particular user session that is executing
// queries in the backend.
message UserContext {
  string user_id = 1;
  string user_name = 2;

  // To extend the existing user context message that is used to identify incoming requests,
  // Spark Connect leverages the Any protobuf type that can be used to inject arbitrary other
  // messages into this message. Extensions are stored as a `repeated` type to be able to
  // handle multiple active extensions.
  repeated google.protobuf.Any extensions = 999;
}

// Request to perform plan analyze, optionally to explain the plan.
message AnalyzePlanRequest {
  // (Required)
  //
  // The client_id is set by the client to be able to collate streaming responses from
  // different queries.
  string client_id = 1;

  // (Required) User context
  UserContext user_context = 2;

  // (Required) The logical plan to be analyzed.
  Plan plan = 3;

  // Provides optional information about the client sending the request. This field
  // can be used for language or version specific information and is only intended for
  // logging purposes and will not be interpreted by the server.
  optional string client_type = 4;

  // (Optional) Get the explain string of the plan.
  Explain explain = 5;
}

// Response to performing analysis of the query. Contains relevant metadata to be able to
// reason about the performance.
message AnalyzePlanResponse {
  string client_id = 1;
  DataType schema = 2;

  // The extended explain string as produced by Spark.
  string explain_string = 3;

  // Get the tree string of the schema.
  string tree_string = 4;

  // Whether the 'collect' and 'take' methods can be run locally.
  bool is_local = 5;

  // Whether this plan contains one or more sources that continuously
  // return data as it arrives.
  bool is_streaming = 6;

  // A best-effort snapshot of the files that compose this Dataset
  repeated string input_files = 7;
}

// A request to be executed by the service.
message ExecutePlanRequest {
  // (Required)
  //
  // The client_id is set by the client to be able to collate streaming responses from
  // different queries.
  string client_id = 1;

  // (Required) User context
  UserContext user_context = 2;

  // (Required) The logical plan to be executed / analyzed.
  Plan plan = 3;

  // Provides optional information about the client sending the request. This field
  // can be used for language or version specific information and is only intended for
  // logging purposes and will not be interpreted by the server.
  optional string client_type = 4;
}

// The response of a query, can be one or more for each request. Responses belonging to the
// same input query, carry the same `client_id`.
message ExecutePlanResponse {
  string client_id = 1;

  ArrowBatch arrow_batch = 2;

  // Metrics for the query execution. Typically, this field is only present in the last
  // batch of results and then represent the overall state of the query execution.
  Metrics metrics = 4;

  // Batch results of metrics.
  message ArrowBatch {
    int64 row_count = 1;
    bytes data = 2;
  }

  message Metrics {

    repeated MetricObject metrics = 1;

    message MetricObject {
      string name = 1;
      int64 plan_id = 2;
      int64 parent = 3;
      map<string, MetricValue> execution_metrics = 4;
    }

    message MetricValue {
      string name = 1;
      int64 value = 2;
      string metric_type = 3;
    }
  }
}

// Main interface for the SparkConnect service.
service SparkConnectService {

  // Executes a request that contains the query and returns a stream of [[Response]].
  //
  // It is guaranteed that there is at least one ARROW batch returned even if the result set is empty.
  rpc ExecutePlan(ExecutePlanRequest) returns (stream ExecutePlanResponse) {}

  // Analyzes a query and returns a [[AnalyzeResponse]] containing metadata about the query.
  rpc AnalyzePlan(AnalyzePlanRequest) returns (AnalyzePlanResponse) {}
}

